{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we explore how to use basic natural languate processing and machine learning techniques to automatically grade essays (AES) in Brazilian Portuguese. In general, AES is a very difficult problem even in English. The available tools that can be used for Portuguese are even less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essay Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question = pd.read_excel(io=\"/home/alin/MyLearning/EasyNLP/Brazil_essay.xlsx\", sheetname=\"Essay Question\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question = question.Original[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conforme apresentado no vídeo da Revista Exame, o comportamento é muito importante no trabalho, nas organizações. Em sua opinião, o comportamento é responsabilidade da própria pessoa ou da empresa em que trabalha? O que as organizações podem fazer para ajudar seus colaboradores a desenvolverem comportamentos melhores e mais adequados às necessidades do trabalho?\n"
     ]
    }
   ],
   "source": [
    "print question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df = pd.read_excel(io=\"/home/alin/MyLearning/EasyNLP/Brazil_essay.xlsx\", sheetname=\"result\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df.columns = essay_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>pk1</th>\n",
       "      <th>title</th>\n",
       "      <th>average_score</th>\n",
       "      <th>resposta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.071078</td>\n",
       "      <td>2500</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt; Em minha opinião que quando se trata de co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.081038</td>\n",
       "      <td>4817</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;Olá Patricia,&lt;/p&gt; \\n&lt;p&gt;Concordo com sua col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.081924</td>\n",
       "      <td>7253</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;O comportamento do profissional dentro da e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.091905</td>\n",
       "      <td>11815</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"font-family: georgia , palatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.101049</td>\n",
       "      <td>704153</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"font-size: 10.0pt;font-family:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id     pk1        title  average_score  \\\n",
       "0  1.071078    2500  ATIVIDADE 1            2.5   \n",
       "1  1.081038    4817  ATIVIDADE 1            2.5   \n",
       "2  1.081924    7253  ATIVIDADE 1            2.5   \n",
       "3  1.091905   11815  ATIVIDADE 1            2.5   \n",
       "4  1.101049  704153  ATIVIDADE 1            1.5   \n",
       "\n",
       "                                            resposta  \n",
       "0  <p> Em minha opinião que quando se trata de co...  \n",
       "1  <p>Olá Patricia,</p> \\n<p>Concordo com sua col...  \n",
       "2  <p>O comportamento do profissional dentro da e...  \n",
       "3  <p><span style=\"font-family: georgia , palatin...  \n",
       "4  <p><span style=\"font-size: 10.0pt;font-family:...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count # of paragraphs and remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "essay_df['paragraphs'] = essay_df.apply(lambda r: len(re.findall(r'<p', r['resposta'])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['text'] = essay_df.apply(lambda r: re.sub(r'<[^<>]*>', ' ', r['resposta']), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>pk1</th>\n",
       "      <th>title</th>\n",
       "      <th>average_score</th>\n",
       "      <th>resposta</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.071078</td>\n",
       "      <td>2500</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt; Em minha opinião que quando se trata de co...</td>\n",
       "      <td>6</td>\n",
       "      <td>Em minha opinião que quando se trata de comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.081038</td>\n",
       "      <td>4817</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;Olá Patricia,&lt;/p&gt; \\n&lt;p&gt;Concordo com sua col...</td>\n",
       "      <td>4</td>\n",
       "      <td>Olá Patricia,  \\n Concordo com sua colocação ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.081924</td>\n",
       "      <td>7253</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;O comportamento do profissional dentro da e...</td>\n",
       "      <td>2</td>\n",
       "      <td>O comportamento do profissional dentro da emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.091905</td>\n",
       "      <td>11815</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"font-family: georgia , palatin...</td>\n",
       "      <td>4</td>\n",
       "      <td>\\n  Em um primeiro momento o comportamen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.101049</td>\n",
       "      <td>704153</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"font-size: 10.0pt;font-family:...</td>\n",
       "      <td>2</td>\n",
       "      <td>Questões comportamentais estão relacionadas ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id     pk1        title  average_score  \\\n",
       "0  1.071078    2500  ATIVIDADE 1            2.5   \n",
       "1  1.081038    4817  ATIVIDADE 1            2.5   \n",
       "2  1.081924    7253  ATIVIDADE 1            2.5   \n",
       "3  1.091905   11815  ATIVIDADE 1            2.5   \n",
       "4  1.101049  704153  ATIVIDADE 1            1.5   \n",
       "\n",
       "                                            resposta  paragraphs  \\\n",
       "0  <p> Em minha opinião que quando se trata de co...           6   \n",
       "1  <p>Olá Patricia,</p> \\n<p>Concordo com sua col...           4   \n",
       "2  <p>O comportamento do profissional dentro da e...           2   \n",
       "3  <p><span style=\"font-family: georgia , palatin...           4   \n",
       "4  <p><span style=\"font-size: 10.0pt;font-family:...           2   \n",
       "\n",
       "                                                text  \n",
       "0    Em minha opinião que quando se trata de comp...  \n",
       "1   Olá Patricia,  \\n Concordo com sua colocação ...  \n",
       "2   O comportamento do profissional dentro da emp...  \n",
       "3        \\n  Em um primeiro momento o comportamen...  \n",
       "4    Questões comportamentais estão relacionadas ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['tokens'] = essay_df.apply(lambda r: nltk.wordpunct_tokenize(r['text']), axis = 1)\n",
    "essay_df['nlp_text'] = essay_df.apply(lambda r: nltk.Text(r['tokens']), axis = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>pk1</th>\n",
       "      <th>title</th>\n",
       "      <th>average_score</th>\n",
       "      <th>resposta</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>nlp_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.071078</td>\n",
       "      <td>2500</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt; Em minha opinião que quando se trata de co...</td>\n",
       "      <td>6</td>\n",
       "      <td>Em minha opinião que quando se trata de comp...</td>\n",
       "      <td>[Em, minha, opinião, que, quando, se, trata, d...</td>\n",
       "      <td>(Em, minha, opinião, que, quando, se, trata, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.081038</td>\n",
       "      <td>4817</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;Olá Patricia,&lt;/p&gt; \\n&lt;p&gt;Concordo com sua col...</td>\n",
       "      <td>4</td>\n",
       "      <td>Olá Patricia,  \\n Concordo com sua colocação ...</td>\n",
       "      <td>[Olá, Patricia, ,, Concordo, com, sua, colocaç...</td>\n",
       "      <td>(Olá, Patricia, ,, Concordo, com, sua, colocaç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.081924</td>\n",
       "      <td>7253</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;O comportamento do profissional dentro da e...</td>\n",
       "      <td>2</td>\n",
       "      <td>O comportamento do profissional dentro da emp...</td>\n",
       "      <td>[O, comportamento, do, profissional, dentro, d...</td>\n",
       "      <td>(O, comportamento, do, profissional, dentro, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.091905</td>\n",
       "      <td>11815</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"font-family: georgia , palatin...</td>\n",
       "      <td>4</td>\n",
       "      <td>\\n  Em um primeiro momento o comportamen...</td>\n",
       "      <td>[Em, um, primeiro, momento, o, comportamento, ...</td>\n",
       "      <td>(Em, um, primeiro, momento, o, comportamento, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.101049</td>\n",
       "      <td>704153</td>\n",
       "      <td>ATIVIDADE 1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>&lt;p&gt;&lt;span style=\"font-size: 10.0pt;font-family:...</td>\n",
       "      <td>2</td>\n",
       "      <td>Questões comportamentais estão relacionadas ...</td>\n",
       "      <td>[Questões, comportamentais, estão, relacionada...</td>\n",
       "      <td>(Questões, comportamentais, estão, relacionada...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id     pk1        title  average_score  \\\n",
       "0  1.071078    2500  ATIVIDADE 1            2.5   \n",
       "1  1.081038    4817  ATIVIDADE 1            2.5   \n",
       "2  1.081924    7253  ATIVIDADE 1            2.5   \n",
       "3  1.091905   11815  ATIVIDADE 1            2.5   \n",
       "4  1.101049  704153  ATIVIDADE 1            1.5   \n",
       "\n",
       "                                            resposta  paragraphs  \\\n",
       "0  <p> Em minha opinião que quando se trata de co...           6   \n",
       "1  <p>Olá Patricia,</p> \\n<p>Concordo com sua col...           4   \n",
       "2  <p>O comportamento do profissional dentro da e...           2   \n",
       "3  <p><span style=\"font-family: georgia , palatin...           4   \n",
       "4  <p><span style=\"font-size: 10.0pt;font-family:...           2   \n",
       "\n",
       "                                                text  \\\n",
       "0    Em minha opinião que quando se trata de comp...   \n",
       "1   Olá Patricia,  \\n Concordo com sua colocação ...   \n",
       "2   O comportamento do profissional dentro da emp...   \n",
       "3        \\n  Em um primeiro momento o comportamen...   \n",
       "4    Questões comportamentais estão relacionadas ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Em, minha, opinião, que, quando, se, trata, d...   \n",
       "1  [Olá, Patricia, ,, Concordo, com, sua, colocaç...   \n",
       "2  [O, comportamento, do, profissional, dentro, d...   \n",
       "3  [Em, um, primeiro, momento, o, comportamento, ...   \n",
       "4  [Questões, comportamentais, estão, relacionada...   \n",
       "\n",
       "                                            nlp_text  \n",
       "0  (Em, minha, opinião, que, quando, se, trata, d...  \n",
       "1  (Olá, Patricia, ,, Concordo, com, sua, colocaç...  \n",
       "2  (O, comportamento, do, profissional, dentro, d...  \n",
       "3  (Em, um, primeiro, momento, o, comportamento, ...  \n",
       "4  (Questões, comportamentais, estão, relacionada...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Character counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['chr_cnt'] = essay_df.apply(lambda r: len(r['text']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token counts (including stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['token_cnt'] = essay_df.apply(lambda r: len(r['tokens']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token counts (excluding stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['tokens_fld'] = essay_df.apply(lambda r: [w.lower() for w in r['tokens'] \n",
    "                                                   if w not in stopwords and not w.isnumeric() and len(w) > 1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['token_cnt_fld'] = essay_df.apply(lambda r: len(r['tokens_fld']), axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of sentences and number of sentences longer than 250 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['sentences'] = essay_df.apply(lambda r: nltk.sent_tokenize(r['text']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['sent_cnt'] = essay_df.apply(lambda r: len(r['sentences']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['long_sent_cnt'] = essay_df.apply(lambda r: len([s for s in r['sentences'] if len(s) > 250]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average length (# of tokens) of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['avg_sent_len'] = essay_df.apply(lambda r: float(r['token_cnt'] / r['sent_cnt']), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of words  that appear both in the question and the essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_token = set([w.lower() for w in nltk.wordpunct_tokenize(question) \n",
    "                      if w not in stopwords and not w.isnumeric() and len(w) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "essay_df['question_tokens'] = essay_df.apply(lambda r: len(set(r['tokens_fld']).intersection(question_token)), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "essay_df['pass'] = np.where(essay_df['average_score'] >= 2, 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df, test_df, _, _ = train_test_split(essay_df, essay_df['pass'], test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_df['pass']\n",
    "z_train = train_df['average_score']\n",
    "y_test = test_df['pass']\n",
    "z_test = test_df['average_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['paragraphs', 'chr_cnt', 'token_cnt', 'token_cnt_fld', 'sent_cnt', 'long_sent_cnt', 'avg_sent_len', 'question_tokens']\n",
    "X_train_1 = train_df[features].values\n",
    "X_test_1 = test_df[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_1lst = [X_train_1[:,i] for i in range(X_train_1.shape[1])]\n",
    "X_test_1lst = [X_test_1[:,i] for  i in range(X_test_1.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta\n",
    "Tagger0 = nltk.DefaultTagger('n')\n",
    "def simplify_tag(t):\n",
    "    if \"+\" in t:\n",
    "        return t[t.index(\"+\")+1:]\n",
    "    else:\n",
    "        return t\n",
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in floresta.tagged_sents() if sent]\n",
    "Tagger1 = nltk.UnigramTagger(tsents, backoff=Tagger0)\n",
    "Tagger2 = nltk.BigramTagger(tsents, backoff=Tagger1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add unigram and bigram pos tag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_unigram = set()\n",
    "all_bigram = set()\n",
    "train_unigrams = []\n",
    "train_bigrams = []\n",
    "for sents in train_df.sentences:\n",
    "    tui = {}\n",
    "    tbi = {}\n",
    "    for sent in sents:\n",
    "        tsent = Tagger2.tag(nltk.word_tokenize(sent))\n",
    "        for i in range(len(tsent) - 1):\n",
    "            t0 = tsent[i][1]\n",
    "            t1 = tsent[i+1][1]\n",
    "            all_unigram.add(t0)\n",
    "            all_bigram.add((t0, t1))\n",
    "            tui[t0] = tui[t0] + 1 if t0 in tui else 1\n",
    "            tbi[(t0, t1)] = tbi[(t0, t1)] + 1 if (t0, t1) in tbi else 1\n",
    "        t0 = tsent[len(tsent) - 1][1]\n",
    "        all_unigram.add(t0)\n",
    "        tui[t0] = tui[t0] + 1 if t0 in tui else 1\n",
    "    train_unigrams.append(tui)\n",
    "    train_bigrams.append(tbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep those tag grams appear in at least 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 5\n",
    "use_cnt = [(x, sum(x in tu for tu in train_unigrams)) for x in all_unigram]\n",
    "use_unigram = set([x for (x,y) in use_cnt if y >L])\n",
    "use_cnt = [(x, sum(x in tb for tb in train_bigrams)) for x in all_bigram]\n",
    "use_bigram = set([x for (x,y) in use_cnt if y >L])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pos tag features of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_unigrams = []\n",
    "test_bigrams = []\n",
    "for sents in test_df.sentences:\n",
    "    tui = {}\n",
    "    tbi = {}\n",
    "    for sent in sents:\n",
    "        tsent = Tagger2.tag(nltk.word_tokenize(sent))\n",
    "        for i in range(len(tsent) - 1):\n",
    "            t0 = tsent[i][1]\n",
    "            t1 = tsent[i+1][1]\n",
    "            if t0 in use_unigram:\n",
    "                tui[t0] = tui[t0] + 1 if t0 in tui else 1\n",
    "            if (t0, t1) in use_bigram:\n",
    "                tbi[(t0, t1)] = tbi[(t0, t1)] + 1 if (t0, t1) in tbi else 1\n",
    "        t0 = tsent[len(tsent) - 1][1]\n",
    "        if t0 in use_unigram:\n",
    "            tui[t0] = tui[t0] + 1 if t0 in tui else 1\n",
    "    test_unigrams.append(tui)\n",
    "    test_bigrams.append(tbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unigram pos tag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unigram0 = [{u: tu[u] if u in tu else 0 for u in use_unigram} for tu in train_unigrams]\n",
    "\n",
    "train_uni_mat = pd.DataFrame(train_unigram0).values\n",
    "\n",
    "test_unigram0 = [{u: tu[u] if u in tu else 0 for u in use_unigram} for tu in test_unigrams]\n",
    "\n",
    "test_uni_mat = pd.DataFrame(test_unigram0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = train_uni_mat\n",
    "X_test_2 = test_uni_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigram pos tag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bigram0 = [{u: tu[u] if u in tu else 0 for u in use_bigram} for tu in train_bigrams]\n",
    "\n",
    "train_bi_mat = pd.DataFrame(train_bigram0).values\n",
    "\n",
    "test_bigram0 = [{u: tu[u] if u in tu else 0 for u in use_bigram} for tu in test_bigrams]\n",
    "\n",
    "test_bi_mat = pd.DataFrame(test_bigram0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_3 = train_bi_mat\n",
    "X_test_3 = test_bi_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = np.array(train_df['text'])\n",
    "test_text = np.array(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vector  ignoring tokens with doc-frequency < 5 and excluding stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words = stopwords).fit(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_4 = vect.transform(train_text)\n",
    "X_test_4 = vect.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf ignoring tokens with doc-frequency < 5 and excluding stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=5, stop_words=stopwords).fit(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_5 = vect.transform(train_text)\n",
    "X_test_5 = vect.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_123 = np.concatenate((X_train_1, X_train_2, X_train_3), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_123 = np.concatenate((X_test_1, X_test_2, X_test_3), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train_1)\n",
    "X_train_1n = scaler.transform(X_train_1)\n",
    "X_test_1n = scaler.transform(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train_123)\n",
    "X_train_123n = scaler.transform(X_train_123)\n",
    "X_test_123n = scaler.transform(X_test_123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_1lst = [X_train_1[:,i] for i in range(X_train_1n.shape[1])]\n",
    "X_test_1lst = [X_test_1[:,i] for  i in range(X_test_1n.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_14 = add_feature(X_train_4, X_train_1lst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_14 = add_feature(X_test_4, X_test_1lst)\n",
    "X_train_15 = add_feature(X_train_5, X_train_1lst)\n",
    "X_test_15 = add_feature(X_test_5, X_test_1lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try some basic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### constant model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879907621247\n",
      "0.346543473107\n"
     ]
    }
   ],
   "source": [
    "z_predict_b = np.ones(y_test.shape[0])*2.5\n",
    "y_predict_b = np.ones(y_test.shape[0])\n",
    "print accuracy_score(y_test, y_predict_b)\n",
    "print np.sqrt(mean_squared_error(y_test, y_predict_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_result(y_true, y_predict):\n",
    "    print accuracy_score(y_true, y_predict)\n",
    "    print f1_score(y_true, y_predict)\n",
    "    print confusion_matrix(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use feature 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'error',\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.133582+0.0079291\ttest-error:0.152239+0.0321809\n",
      "[1]\ttrain-error:0.13209+0.00884382\ttest-error:0.135323+0.0312127\n",
      "[2]\ttrain-error:0.132836+0.00709709\ttest-error:0.136318+0.0302789\n",
      "[3]\ttrain-error:0.133333+0.00760219\ttest-error:0.136318+0.0312444\n",
      "[4]\ttrain-error:0.131094+0.00660948\ttest-error:0.137314+0.0314024\n",
      "[5]\ttrain-error:0.131095+0.00714949\ttest-error:0.139304+0.0295172\n",
      "[6]\ttrain-error:0.13209+0.00616393\ttest-error:0.139304+0.0295172\n",
      "[7]\ttrain-error:0.131841+0.00583373\ttest-error:0.136318+0.0296177\n",
      "[8]\ttrain-error:0.131841+0.00567237\ttest-error:0.136318+0.0296177\n",
      "[9]\ttrain-error:0.132089+0.00574819\ttest-error:0.137313+0.0294501\n",
      "[10]\ttrain-error:0.131592+0.0058549\ttest-error:0.135323+0.0304094\n",
      "[11]\ttrain-error:0.130099+0.00591788\ttest-error:0.136318+0.0302789\n",
      "[12]\ttrain-error:0.130597+0.00561754\ttest-error:0.137313+0.0302788\n",
      "[13]\ttrain-error:0.130348+0.0054156\ttest-error:0.137313+0.0302788\n",
      "[14]\ttrain-error:0.130846+0.0055287\ttest-error:0.137313+0.0289414\n",
      "[15]\ttrain-error:0.130597+0.00561754\ttest-error:0.137313+0.0289414\n",
      "[16]\ttrain-error:0.129851+0.00488721\ttest-error:0.136318+0.0296177\n",
      "[17]\ttrain-error:0.129353+0.00471991\ttest-error:0.136318+0.0296177\n",
      "[18]\ttrain-error:0.129353+0.00471991\ttest-error:0.136318+0.0296177\n",
      "[19]\ttrain-error:0.128607+0.00469335\ttest-error:0.135323+0.0304094\n",
      "[20]\ttrain-error:0.1301+0.00513422\ttest-error:0.135323+0.0304094\n",
      "[21]\ttrain-error:0.1301+0.00513422\ttest-error:0.136318+0.0302789\n",
      "[22]\ttrain-error:0.130099+0.00525325\ttest-error:0.136318+0.0302789\n",
      "[23]\ttrain-error:0.129851+0.00507359\ttest-error:0.135323+0.0312127\n",
      "[24]\ttrain-error:0.128856+0.00455986\ttest-error:0.136318+0.0302789\n",
      "[25]\ttrain-error:0.128358+0.00493757\ttest-error:0.136318+0.0302789\n",
      "[26]\ttrain-error:0.128607+0.00475886\ttest-error:0.135323+0.0304094\n",
      "[27]\ttrain-error:0.128109+0.00458689\ttest-error:0.135323+0.0304094\n",
      "[28]\ttrain-error:0.127612+0.00405702\ttest-error:0.136318+0.0302789\n",
      "[29]\ttrain-error:0.125871+0.00580181\ttest-error:0.136318+0.0302789\n",
      "[30]\ttrain-error:0.125871+0.00558462\ttest-error:0.136318+0.0302789\n",
      "[31]\ttrain-error:0.125622+0.00478483\ttest-error:0.135323+0.0304094\n",
      "[32]\ttrain-error:0.125373+0.00547245\ttest-error:0.135323+0.0304094\n",
      "[33]\ttrain-error:0.125622+0.00572668\ttest-error:0.135323+0.0304094\n",
      "[34]\ttrain-error:0.125124+0.00591799\ttest-error:0.135323+0.0304094\n",
      "[35]\ttrain-error:0.125124+0.00627327\ttest-error:0.135323+0.0304094\n",
      "[36]\ttrain-error:0.124876+0.00602157\ttest-error:0.136318+0.0302789\n",
      "[37]\ttrain-error:0.124378+0.00567237\ttest-error:0.136318+0.0302789\n",
      "[38]\ttrain-error:0.124129+0.0055844\ttest-error:0.137313+0.0296177\n",
      "[39]\ttrain-error:0.123383+0.00574825\ttest-error:0.137313+0.0296177\n",
      "[40]\ttrain-error:0.122885+0.00626348\ttest-error:0.137313+0.0302788\n",
      "[41]\ttrain-error:0.122637+0.00602167\ttest-error:0.136318+0.0302789\n",
      "[42]\ttrain-error:0.121144+0.00632235\ttest-error:0.137313+0.0302788\n",
      "[43]\ttrain-error:0.120896+0.00541603\ttest-error:0.138308+0.0295842\n",
      "[44]\ttrain-error:0.120398+0.00650566\ttest-error:0.139303+0.0286664\n",
      "[45]\ttrain-error:0.118657+0.00548375\ttest-error:0.139303+0.0286664\n",
      "[46]\ttrain-error:0.118657+0.00501202\ttest-error:0.139303+0.0286664\n",
      "[47]\ttrain-error:0.118408+0.00461347\ttest-error:0.139303+0.0286664\n",
      "[48]\ttrain-error:0.117662+0.00507355\ttest-error:0.138308+0.0295842\n",
      "[49]\ttrain-error:0.117413+0.00482363\ttest-error:0.138308+0.0295842\n",
      "[50]\ttrain-error:0.116169+0.00570518\ttest-error:0.140299+0.0287354\n",
      "[51]\ttrain-error:0.113433+0.00611346\ttest-error:0.142289+0.0299501\n",
      "[52]\ttrain-error:0.113184+0.00716671\ttest-error:0.141293+0.0299501\n",
      "[53]\ttrain-error:0.113184+0.00634204\ttest-error:0.142289+0.0299501\n",
      "[54]\ttrain-error:0.112687+0.00727811\ttest-error:0.143284+0.0290779\n",
      "[55]\ttrain-error:0.111692+0.00855956\ttest-error:0.142289+0.0299501\n",
      "[56]\ttrain-error:0.110448+0.00776331\ttest-error:0.143284+0.0290779\n",
      "[57]\ttrain-error:0.110199+0.00723515\ttest-error:0.144279+0.0286664\n",
      "[58]\ttrain-error:0.108707+0.00688488\ttest-error:0.145274+0.0280379\n",
      "[59]\ttrain-error:0.107712+0.00591798\ttest-error:0.144279+0.0286664\n"
     ]
    }
   ],
   "source": [
    "cvresult = xgb.cv(xgb_params, dtrain, num_boost_round=1000, nfold=5,\n",
    "            metrics='error', early_stopping_rounds=50, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rounds = 59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.134787\n",
      "[20]\ttrain-error:0.133796\n",
      "[40]\ttrain-error:0.129832\n",
      "[58]\ttrain-error:0.115956\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(xgb_params, dtrain, num_boost_round = num_rounds, evals = [(dtrain, 'train')], verbose_eval = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred1 = model.predict(dtest)\n",
    "predictions17 = [1 if x >=0.5 else 0 for x in y_pred1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875288683603\n",
      "0.933497536946\n",
      "[[  0  52]\n",
      " [  2 379]]\n"
     ]
    }
   ],
   "source": [
    "check_result(y_test, predictions17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train_1, z_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params1 = {\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "     'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.66595+0.00464818\ttest-rmse:1.66588+0.0195561\n",
      "[1]\ttrain-rmse:1.50823+0.00417447\ttest-rmse:1.50886+0.0198445\n",
      "[2]\ttrain-rmse:1.36685+0.0033935\ttest-rmse:1.36752+0.0199878\n",
      "[3]\ttrain-rmse:1.24027+0.00278493\ttest-rmse:1.24136+0.0200829\n",
      "[4]\ttrain-rmse:1.12748+0.00274655\ttest-rmse:1.1293+0.0191711\n",
      "[5]\ttrain-rmse:1.02724+0.00243613\ttest-rmse:1.03017+0.0189689\n",
      "[6]\ttrain-rmse:0.93724+0.00162578\ttest-rmse:0.941997+0.019365\n",
      "[7]\ttrain-rmse:0.857745+0.00116613\ttest-rmse:0.864594+0.0178489\n",
      "[8]\ttrain-rmse:0.786553+0.00113758\ttest-rmse:0.794593+0.0166307\n",
      "[9]\ttrain-rmse:0.723274+0.00170383\ttest-rmse:0.732566+0.015698\n",
      "[10]\ttrain-rmse:0.667728+0.00201862\ttest-rmse:0.679702+0.0153046\n",
      "[11]\ttrain-rmse:0.618633+0.0024341\ttest-rmse:0.633003+0.0137845\n",
      "[12]\ttrain-rmse:0.576014+0.00238111\ttest-rmse:0.592497+0.012945\n",
      "[13]\ttrain-rmse:0.538097+0.00282257\ttest-rmse:0.557894+0.0115237\n",
      "[14]\ttrain-rmse:0.504966+0.00301833\ttest-rmse:0.528316+0.0103312\n",
      "[15]\ttrain-rmse:0.476052+0.00340289\ttest-rmse:0.503545+0.00995625\n",
      "[16]\ttrain-rmse:0.450854+0.00384576\ttest-rmse:0.481404+0.00870956\n",
      "[17]\ttrain-rmse:0.429328+0.00433998\ttest-rmse:0.463316+0.00722498\n",
      "[18]\ttrain-rmse:0.410429+0.00445308\ttest-rmse:0.448539+0.00729133\n",
      "[19]\ttrain-rmse:0.393795+0.00466217\ttest-rmse:0.43559+0.00710989\n",
      "[20]\ttrain-rmse:0.379666+0.00487772\ttest-rmse:0.424385+0.00756333\n",
      "[21]\ttrain-rmse:0.367489+0.0051445\ttest-rmse:0.415054+0.00796618\n",
      "[22]\ttrain-rmse:0.356673+0.00515909\ttest-rmse:0.407828+0.00956733\n",
      "[23]\ttrain-rmse:0.347286+0.00543931\ttest-rmse:0.401813+0.0104352\n",
      "[24]\ttrain-rmse:0.339327+0.0056668\ttest-rmse:0.397158+0.0114443\n",
      "[25]\ttrain-rmse:0.332211+0.00631244\ttest-rmse:0.393424+0.0128058\n",
      "[26]\ttrain-rmse:0.326552+0.00621566\ttest-rmse:0.389383+0.014259\n",
      "[27]\ttrain-rmse:0.321496+0.00637541\ttest-rmse:0.38707+0.0157274\n",
      "[28]\ttrain-rmse:0.316681+0.0064448\ttest-rmse:0.385732+0.0160589\n",
      "[29]\ttrain-rmse:0.312023+0.00632402\ttest-rmse:0.384692+0.0171642\n",
      "[30]\ttrain-rmse:0.308043+0.00637512\ttest-rmse:0.383733+0.0176314\n",
      "[31]\ttrain-rmse:0.305113+0.00622099\ttest-rmse:0.382915+0.0181017\n",
      "[32]\ttrain-rmse:0.302691+0.00609336\ttest-rmse:0.382266+0.0189592\n",
      "[33]\ttrain-rmse:0.300145+0.00659951\ttest-rmse:0.381586+0.0196227\n",
      "[34]\ttrain-rmse:0.298458+0.00665216\ttest-rmse:0.381349+0.019986\n",
      "[35]\ttrain-rmse:0.296641+0.00665151\ttest-rmse:0.381488+0.0203125\n",
      "[36]\ttrain-rmse:0.294379+0.00632065\ttest-rmse:0.381716+0.0207009\n",
      "[37]\ttrain-rmse:0.292238+0.00640413\ttest-rmse:0.382063+0.0206258\n",
      "[38]\ttrain-rmse:0.290336+0.00603206\ttest-rmse:0.381764+0.02071\n",
      "[39]\ttrain-rmse:0.288754+0.00633333\ttest-rmse:0.381954+0.0209115\n",
      "[40]\ttrain-rmse:0.286639+0.00683773\ttest-rmse:0.382452+0.0214721\n",
      "[41]\ttrain-rmse:0.284182+0.00711693\ttest-rmse:0.382574+0.0213655\n",
      "[42]\ttrain-rmse:0.282509+0.00735282\ttest-rmse:0.382488+0.0213997\n",
      "[43]\ttrain-rmse:0.281028+0.00741307\ttest-rmse:0.382695+0.0215417\n",
      "[44]\ttrain-rmse:0.279104+0.00677388\ttest-rmse:0.382726+0.0217671\n",
      "[45]\ttrain-rmse:0.277242+0.0069082\ttest-rmse:0.383028+0.0216609\n",
      "[46]\ttrain-rmse:0.275579+0.00674241\ttest-rmse:0.383538+0.021774\n",
      "[47]\ttrain-rmse:0.274073+0.00663632\ttest-rmse:0.3837+0.0217339\n",
      "[48]\ttrain-rmse:0.273028+0.00654729\ttest-rmse:0.383939+0.0222057\n",
      "[49]\ttrain-rmse:0.271519+0.00639624\ttest-rmse:0.384196+0.0224152\n",
      "[50]\ttrain-rmse:0.269662+0.00639773\ttest-rmse:0.385226+0.0225437\n",
      "[51]\ttrain-rmse:0.268645+0.00627452\ttest-rmse:0.385622+0.0227984\n",
      "[52]\ttrain-rmse:0.266876+0.00584287\ttest-rmse:0.385821+0.023129\n",
      "[53]\ttrain-rmse:0.265875+0.00597962\ttest-rmse:0.38635+0.0236534\n",
      "[54]\ttrain-rmse:0.264579+0.00621323\ttest-rmse:0.386622+0.0236144\n",
      "[55]\ttrain-rmse:0.262718+0.00639301\ttest-rmse:0.387091+0.024315\n",
      "[56]\ttrain-rmse:0.261337+0.00611845\ttest-rmse:0.387873+0.0241405\n",
      "[57]\ttrain-rmse:0.260018+0.00657276\ttest-rmse:0.388411+0.0240871\n",
      "[58]\ttrain-rmse:0.258739+0.00625194\ttest-rmse:0.389124+0.0240585\n",
      "[59]\ttrain-rmse:0.257127+0.00627515\ttest-rmse:0.389577+0.0245156\n",
      "[60]\ttrain-rmse:0.255662+0.00631785\ttest-rmse:0.390114+0.0247577\n",
      "[61]\ttrain-rmse:0.254601+0.00655588\ttest-rmse:0.390952+0.0249282\n",
      "[62]\ttrain-rmse:0.25362+0.00673242\ttest-rmse:0.391194+0.0246643\n",
      "[63]\ttrain-rmse:0.252349+0.00642133\ttest-rmse:0.391355+0.024925\n",
      "[64]\ttrain-rmse:0.250829+0.00610658\ttest-rmse:0.391625+0.0247214\n",
      "[65]\ttrain-rmse:0.249758+0.00620703\ttest-rmse:0.391892+0.0250774\n",
      "[66]\ttrain-rmse:0.248249+0.00595743\ttest-rmse:0.391896+0.0250826\n",
      "[67]\ttrain-rmse:0.246756+0.00595706\ttest-rmse:0.392293+0.0247317\n",
      "[68]\ttrain-rmse:0.245751+0.00634481\ttest-rmse:0.39247+0.0246977\n",
      "[69]\ttrain-rmse:0.244378+0.00589506\ttest-rmse:0.392891+0.0246204\n",
      "[70]\ttrain-rmse:0.242713+0.00588168\ttest-rmse:0.393092+0.0245593\n",
      "[71]\ttrain-rmse:0.241449+0.00580048\ttest-rmse:0.393386+0.0246382\n",
      "[72]\ttrain-rmse:0.239871+0.00617508\ttest-rmse:0.393582+0.0244145\n",
      "[73]\ttrain-rmse:0.238614+0.0060107\ttest-rmse:0.393793+0.0245806\n",
      "[74]\ttrain-rmse:0.237128+0.00586877\ttest-rmse:0.394045+0.0251211\n",
      "[75]\ttrain-rmse:0.235465+0.00557116\ttest-rmse:0.394257+0.0248591\n",
      "[76]\ttrain-rmse:0.234045+0.00530057\ttest-rmse:0.394718+0.0252797\n",
      "[77]\ttrain-rmse:0.232452+0.00520546\ttest-rmse:0.395258+0.0254939\n",
      "[78]\ttrain-rmse:0.231621+0.00512148\ttest-rmse:0.395376+0.0254557\n",
      "[79]\ttrain-rmse:0.230343+0.00509349\ttest-rmse:0.395292+0.0251315\n",
      "[80]\ttrain-rmse:0.229206+0.00515284\ttest-rmse:0.395437+0.0254865\n",
      "[81]\ttrain-rmse:0.227625+0.00489305\ttest-rmse:0.395922+0.0254438\n",
      "[82]\ttrain-rmse:0.226566+0.0051264\ttest-rmse:0.395856+0.0252008\n",
      "[83]\ttrain-rmse:0.225384+0.00531871\ttest-rmse:0.395709+0.0253061\n"
     ]
    }
   ],
   "source": [
    "cvresult = xgb.cv(xgb_params1, dtrain, num_boost_round=1000, nfold=5,\n",
    "            metrics='rmse', early_stopping_rounds=50, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.66729\n",
      "[20]\ttrain-rmse:0.38721\n",
      "[40]\ttrain-rmse:0.295882\n",
      "[60]\ttrain-rmse:0.271656\n",
      "[80]\ttrain-rmse:0.250682\n",
      "[82]\ttrain-rmse:0.248841\n",
      "0.36847334743\n",
      "0.854503464203\n",
      "0.92095357591\n",
      "[[  3  49]\n",
      " [ 14 367]]\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 83\n",
    "model = xgb.train(xgb_params1, dtrain, num_boost_round = num_rounds, evals = [(dtrain, 'train')], verbose_eval = 20)\n",
    "z_pred1 = model.predict(dtest)\n",
    "print np.sqrt(mean_squared_error(z_test, z_pred1))\n",
    "predictions18 = [1 if x >=2 else 0 for x in z_pred1]\n",
    "check_result(y_test, predictions18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use features 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.126866+0.00854522\ttest-error:0.171144+0.03324\n",
      "[1]\ttrain-error:0.120647+0.00907218\ttest-error:0.139303+0.0343246\n",
      "[2]\ttrain-error:0.120398+0.0105948\ttest-error:0.140298+0.0308939\n",
      "[3]\ttrain-error:0.122886+0.00887861\ttest-error:0.140298+0.0316849\n",
      "[4]\ttrain-error:0.123383+0.0084502\ttest-error:0.140298+0.0307332\n",
      "[5]\ttrain-error:0.124627+0.00852337\ttest-error:0.135323+0.0287354\n",
      "[6]\ttrain-error:0.124129+0.00974264\ttest-error:0.135323+0.0287354\n",
      "[7]\ttrain-error:0.124876+0.00701827\ttest-error:0.136318+0.0306041\n",
      "[8]\ttrain-error:0.124876+0.00548397\ttest-error:0.137313+0.0271772\n",
      "[9]\ttrain-error:0.124129+0.00611368\ttest-error:0.140298+0.0287354\n",
      "[10]\ttrain-error:0.121144+0.00570514\ttest-error:0.138308+0.029917\n",
      "[11]\ttrain-error:0.122388+0.00559563\ttest-error:0.139303+0.0288386\n",
      "[12]\ttrain-error:0.121891+0.00588667\ttest-error:0.139303+0.0288386\n",
      "[13]\ttrain-error:0.119901+0.00651506\ttest-error:0.139303+0.0288386\n",
      "[14]\ttrain-error:0.119154+0.00722674\ttest-error:0.139303+0.0286664\n",
      "[15]\ttrain-error:0.117911+0.00481055\ttest-error:0.137313+0.0296177\n",
      "[16]\ttrain-error:0.117662+0.00697381\ttest-error:0.137313+0.0296177\n",
      "[17]\ttrain-error:0.115174+0.00622389\ttest-error:0.136318+0.0302789\n",
      "[18]\ttrain-error:0.114179+0.00650589\ttest-error:0.135323+0.0312127\n",
      "[19]\ttrain-error:0.112438+0.00390138\ttest-error:0.135323+0.0312127\n",
      "[20]\ttrain-error:0.108707+0.00449131\ttest-error:0.136318+0.0306041\n",
      "[21]\ttrain-error:0.109453+0.00478483\ttest-error:0.136318+0.0306041\n",
      "[22]\ttrain-error:0.108955+0.00435143\ttest-error:0.136318+0.0306041\n",
      "[23]\ttrain-error:0.108209+0.00497515\ttest-error:0.137313+0.0304419\n",
      "[24]\ttrain-error:0.106219+0.00525361\ttest-error:0.136318+0.0306041\n",
      "[25]\ttrain-error:0.103731+0.00501221\ttest-error:0.136318+0.0306041\n",
      "[26]\ttrain-error:0.102736+0.00488728\ttest-error:0.136318+0.0306041\n",
      "[27]\ttrain-error:0.0997514+0.00411761\ttest-error:0.136318+0.0306041\n",
      "[28]\ttrain-error:0.0977614+0.00482353\ttest-error:0.136318+0.0306041\n",
      "[29]\ttrain-error:0.0935324+0.00337447\ttest-error:0.136318+0.0306041\n",
      "[30]\ttrain-error:0.0902984+0.00300577\ttest-error:0.136318+0.0306041\n",
      "[31]\ttrain-error:0.087562+0.00442199\ttest-error:0.136318+0.0306041\n",
      "[32]\ttrain-error:0.084826+0.00563953\ttest-error:0.136318+0.0306041\n",
      "[33]\ttrain-error:0.080846+0.00766714\ttest-error:0.136318+0.0306041\n",
      "[34]\ttrain-error:0.0798508+0.00541589\ttest-error:0.136318+0.0306041\n",
      "[35]\ttrain-error:0.0763682+0.00679415\ttest-error:0.136318+0.0306041\n",
      "[36]\ttrain-error:0.0736318+0.00640989\ttest-error:0.136318+0.0306041\n",
      "[37]\ttrain-error:0.0713928+0.00892072\ttest-error:0.136318+0.0306041\n",
      "[38]\ttrain-error:0.0679104+0.0101229\ttest-error:0.136318+0.0306041\n",
      "[39]\ttrain-error:0.066418+0.0102445\ttest-error:0.136318+0.0306041\n",
      "[40]\ttrain-error:0.064179+0.00892057\ttest-error:0.136318+0.0306041\n",
      "[41]\ttrain-error:0.0611938+0.0081142\ttest-error:0.136318+0.0306041\n",
      "[42]\ttrain-error:0.0574628+0.00747917\ttest-error:0.136318+0.0306041\n",
      "[43]\ttrain-error:0.0569652+0.00780315\ttest-error:0.136318+0.0306041\n",
      "[44]\ttrain-error:0.0542288+0.00781113\ttest-error:0.136318+0.0306041\n",
      "[45]\ttrain-error:0.0514926+0.00602157\ttest-error:0.136318+0.0306041\n",
      "[46]\ttrain-error:0.0492534+0.00674852\ttest-error:0.136318+0.0306041\n",
      "[47]\ttrain-error:0.0482586+0.00776346\ttest-error:0.137313+0.0296177\n",
      "[48]\ttrain-error:0.0450248+0.00772332\ttest-error:0.135323+0.0316851\n",
      "[49]\ttrain-error:0.0430348+0.00736265\ttest-error:0.136318+0.0306041\n",
      "[50]\ttrain-error:0.0420396+0.00683048\ttest-error:0.135323+0.0316851\n",
      "[51]\ttrain-error:0.0398012+0.00758623\ttest-error:0.135323+0.0316851\n",
      "[52]\ttrain-error:0.037562+0.00722674\ttest-error:0.135323+0.0316851\n",
      "[53]\ttrain-error:0.0373134+0.00737912\ttest-error:0.135323+0.0316851\n",
      "[54]\ttrain-error:0.0355724+0.00670269\ttest-error:0.135323+0.0316851\n",
      "[55]\ttrain-error:0.0323384+0.00572702\ttest-error:0.135323+0.0316851\n",
      "[56]\ttrain-error:0.0308456+0.00506156\ttest-error:0.135323+0.0316851\n",
      "[57]\ttrain-error:0.028358+0.00454604\ttest-error:0.135323+0.0316851\n",
      "[58]\ttrain-error:0.0263682+0.00506157\ttest-error:0.137313+0.0296177\n",
      "[59]\ttrain-error:0.025373+0.0050124\ttest-error:0.137313+0.0296177\n",
      "[60]\ttrain-error:0.0241294+0.00455962\ttest-error:0.136318+0.0306041\n",
      "[61]\ttrain-error:0.0231344+0.00435165\ttest-error:0.136318+0.0306041\n",
      "[62]\ttrain-error:0.021393+0.00440806\ttest-error:0.136318+0.0306041\n",
      "[63]\ttrain-error:0.0196516+0.00328147\ttest-error:0.136318+0.0306041\n",
      "[64]\ttrain-error:0.0194028+0.00290104\ttest-error:0.136318+0.0306041\n",
      "[65]\ttrain-error:0.016169+0.00342889\ttest-error:0.136318+0.0306041\n",
      "[66]\ttrain-error:0.016169+0.00304659\ttest-error:0.136318+0.0306041\n",
      "[67]\ttrain-error:0.0146766+0.00346462\ttest-error:0.136318+0.0306041\n",
      "[68]\ttrain-error:0.0129352+0.00382141\ttest-error:0.136318+0.0306041\n",
      "[69]\ttrain-error:0.0114426+0.00328153\ttest-error:0.135323+0.0316851\n",
      "[70]\ttrain-error:0.0101988+0.00404191\ttest-error:0.135323+0.0316851\n",
      "[71]\ttrain-error:0.0094526+0.00365588\ttest-error:0.135323+0.0316851\n",
      "[72]\ttrain-error:0.0092038+0.00405711\ttest-error:0.135323+0.0316851\n",
      "[73]\ttrain-error:0.0089552+0.00411762\ttest-error:0.136318+0.0306041\n",
      "[74]\ttrain-error:0.007214+0.00346501\ttest-error:0.137313+0.0296177\n",
      "[75]\ttrain-error:0.0062188+0.00377284\ttest-error:0.135323+0.0316851\n",
      "[76]\ttrain-error:0.0059702+0.0043374\ttest-error:0.135323+0.0316851\n",
      "[77]\ttrain-error:0.0049754+0.00408741\ttest-error:0.136318+0.0306041\n",
      "[78]\ttrain-error:0.005224+0.00396452\ttest-error:0.135323+0.0316851\n",
      "[79]\ttrain-error:0.004229+0.00382137\ttest-error:0.137313+0.0301149\n",
      "[80]\ttrain-error:0.0039802+0.00396444\ttest-error:0.137313+0.0301149\n",
      "[81]\ttrain-error:0.0037314+0.00351779\ttest-error:0.136318+0.0306041\n",
      "[82]\ttrain-error:0.003234+0.00348243\ttest-error:0.136318+0.0306041\n",
      "[83]\ttrain-error:0.0027364+0.00308675\ttest-error:0.136318+0.0306041\n",
      "[84]\ttrain-error:0.0024878+0.00260906\ttest-error:0.136318+0.0306041\n",
      "[85]\ttrain-error:0.0019902+0.0029011\ttest-error:0.136318+0.0306041\n",
      "[86]\ttrain-error:0.0019902+0.00230691\ttest-error:0.137313+0.0301149\n",
      "[87]\ttrain-error:0.0014926+0.00182792\ttest-error:0.138308+0.0290779\n",
      "[88]\ttrain-error:0.0017414+0.00230689\ttest-error:0.138308+0.0290779\n",
      "[89]\ttrain-error:0.0014926+0.00145043\ttest-error:0.138308+0.0290779\n",
      "[90]\ttrain-error:0.0017414+0.0018615\ttest-error:0.138308+0.0290779\n",
      "[91]\ttrain-error:0.0014926+0.00182792\ttest-error:0.137313+0.0296177\n",
      "[92]\ttrain-error:0.0014926+0.00182792\ttest-error:0.137313+0.0296177\n",
      "[93]\ttrain-error:0.0014926+0.00182792\ttest-error:0.138308+0.0287354\n",
      "[94]\ttrain-error:0.0014926+0.00182792\ttest-error:0.138308+0.0287354\n",
      "[95]\ttrain-error:0.0012438+0.00192681\ttest-error:0.138308+0.0287354\n",
      "[96]\ttrain-error:0.000995+0.00145036\ttest-error:0.138308+0.0287354\n",
      "[97]\ttrain-error:0.0004976+0.0009952\ttest-error:0.138308+0.0287354\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train_123n, y_train)\n",
    "dtest = xgb.DMatrix(X_test_123n)\n",
    "cvresult = xgb.cv(xgb_params, dtrain, num_boost_round=1000, nfold=5,\n",
    "            metrics='error', early_stopping_rounds=50, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-error:0.124876\n",
      "[20]\ttrain-error:0.119921\n",
      "[40]\ttrain-error:0.084242\n",
      "[60]\ttrain-error:0.039643\n",
      "[80]\ttrain-error:0.013875\n",
      "[96]\ttrain-error:0.005946\n",
      "0.877598152425\n",
      "0.934809348093\n",
      "[[  0  52]\n",
      " [  1 380]]\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 97\n",
    "model = xgb.train(xgb_params, dtrain, num_boost_round = num_rounds, evals = [(dtrain, 'train')], verbose_eval = 20)\n",
    "y_pred1 = model.predict(dtest)\n",
    "predictions18 = [1 if x >=0.5 else 0 for x in y_pred1]\n",
    "check_result(y_test, predictions18 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.66588+0.00464602\ttest-rmse:1.66722+0.0208145\n",
      "[1]\ttrain-rmse:1.50767+0.00383587\ttest-rmse:1.51074+0.0218353\n",
      "[2]\ttrain-rmse:1.36542+0.00335035\ttest-rmse:1.36961+0.0213826\n",
      "[3]\ttrain-rmse:1.23768+0.0025824\ttest-rmse:1.24345+0.0221618\n",
      "[4]\ttrain-rmse:1.12358+0.00211448\ttest-rmse:1.13245+0.0220914\n",
      "[5]\ttrain-rmse:1.02201+0.00153107\ttest-rmse:1.03259+0.0217388\n",
      "[6]\ttrain-rmse:0.930747+0.00180147\ttest-rmse:0.943324+0.0201998\n",
      "[7]\ttrain-rmse:0.849272+0.0018109\ttest-rmse:0.865748+0.0197805\n",
      "[8]\ttrain-rmse:0.776914+0.00194621\ttest-rmse:0.79606+0.0194041\n",
      "[9]\ttrain-rmse:0.712079+0.0019652\ttest-rmse:0.735464+0.0181199\n",
      "[10]\ttrain-rmse:0.654345+0.00225153\ttest-rmse:0.681601+0.0168056\n",
      "[11]\ttrain-rmse:0.602867+0.0022292\ttest-rmse:0.635026+0.016976\n",
      "[12]\ttrain-rmse:0.5583+0.00233603\ttest-rmse:0.595095+0.0150236\n",
      "[13]\ttrain-rmse:0.517275+0.0020376\ttest-rmse:0.560159+0.0131609\n",
      "[14]\ttrain-rmse:0.48184+0.00218133\ttest-rmse:0.529921+0.0119737\n",
      "[15]\ttrain-rmse:0.450327+0.00221836\ttest-rmse:0.502903+0.0103443\n",
      "[16]\ttrain-rmse:0.422432+0.00265036\ttest-rmse:0.481024+0.00906005\n",
      "[17]\ttrain-rmse:0.39774+0.00269231\ttest-rmse:0.462719+0.00869914\n",
      "[18]\ttrain-rmse:0.37642+0.00224728\ttest-rmse:0.446766+0.00848608\n",
      "[19]\ttrain-rmse:0.357386+0.00224475\ttest-rmse:0.433057+0.00844576\n",
      "[20]\ttrain-rmse:0.340484+0.00257238\ttest-rmse:0.422788+0.00813521\n",
      "[21]\ttrain-rmse:0.324755+0.00268958\ttest-rmse:0.413043+0.00861372\n",
      "[22]\ttrain-rmse:0.311854+0.00340171\ttest-rmse:0.405397+0.00898852\n",
      "[23]\ttrain-rmse:0.299703+0.00326884\ttest-rmse:0.398721+0.0102257\n",
      "[24]\ttrain-rmse:0.28957+0.00297416\ttest-rmse:0.392744+0.0111936\n",
      "[25]\ttrain-rmse:0.280263+0.00367721\ttest-rmse:0.388035+0.0121334\n",
      "[26]\ttrain-rmse:0.272041+0.00366142\ttest-rmse:0.384116+0.0132662\n",
      "[27]\ttrain-rmse:0.264669+0.00393416\ttest-rmse:0.381049+0.0138229\n",
      "[28]\ttrain-rmse:0.257553+0.00409878\ttest-rmse:0.37906+0.0148278\n",
      "[29]\ttrain-rmse:0.252319+0.00442641\ttest-rmse:0.377185+0.0155106\n",
      "[30]\ttrain-rmse:0.247134+0.00436776\ttest-rmse:0.37557+0.0163219\n",
      "[31]\ttrain-rmse:0.242209+0.0046564\ttest-rmse:0.374078+0.0173747\n",
      "[32]\ttrain-rmse:0.237893+0.00419326\ttest-rmse:0.373321+0.017499\n",
      "[33]\ttrain-rmse:0.233349+0.00361482\ttest-rmse:0.372475+0.0171296\n",
      "[34]\ttrain-rmse:0.229404+0.00367518\ttest-rmse:0.372327+0.0174442\n",
      "[35]\ttrain-rmse:0.225948+0.00366321\ttest-rmse:0.371878+0.0178998\n",
      "[36]\ttrain-rmse:0.222356+0.00371745\ttest-rmse:0.371821+0.0183749\n",
      "[37]\ttrain-rmse:0.219183+0.00440929\ttest-rmse:0.371969+0.0185608\n",
      "[38]\ttrain-rmse:0.21579+0.00437006\ttest-rmse:0.371491+0.0187652\n",
      "[39]\ttrain-rmse:0.212733+0.00394437\ttest-rmse:0.371318+0.0190199\n",
      "[40]\ttrain-rmse:0.21047+0.00385276\ttest-rmse:0.370975+0.0192516\n",
      "[41]\ttrain-rmse:0.207584+0.00378473\ttest-rmse:0.370406+0.0195465\n",
      "[42]\ttrain-rmse:0.204809+0.0036183\ttest-rmse:0.370397+0.0196669\n",
      "[43]\ttrain-rmse:0.202898+0.003539\ttest-rmse:0.370529+0.0202618\n",
      "[44]\ttrain-rmse:0.200476+0.00317095\ttest-rmse:0.370585+0.0204041\n",
      "[45]\ttrain-rmse:0.198206+0.00342689\ttest-rmse:0.37017+0.0203929\n",
      "[46]\ttrain-rmse:0.195492+0.00370953\ttest-rmse:0.370723+0.0202373\n",
      "[47]\ttrain-rmse:0.192395+0.00380948\ttest-rmse:0.370551+0.0202762\n",
      "[48]\ttrain-rmse:0.190076+0.00373373\ttest-rmse:0.370567+0.0196689\n",
      "[49]\ttrain-rmse:0.187752+0.00390679\ttest-rmse:0.370954+0.020246\n",
      "[50]\ttrain-rmse:0.185127+0.00368123\ttest-rmse:0.370883+0.0201526\n",
      "[51]\ttrain-rmse:0.182488+0.00392497\ttest-rmse:0.370875+0.0199475\n",
      "[52]\ttrain-rmse:0.179814+0.00466771\ttest-rmse:0.371381+0.0197892\n",
      "[53]\ttrain-rmse:0.177973+0.00461138\ttest-rmse:0.371551+0.0201701\n",
      "[54]\ttrain-rmse:0.175323+0.00497338\ttest-rmse:0.371601+0.0204864\n",
      "[55]\ttrain-rmse:0.173374+0.00449203\ttest-rmse:0.371115+0.0205012\n",
      "[56]\ttrain-rmse:0.17169+0.00513933\ttest-rmse:0.370853+0.020726\n",
      "[57]\ttrain-rmse:0.169703+0.00506621\ttest-rmse:0.371067+0.0206877\n",
      "[58]\ttrain-rmse:0.16834+0.0052263\ttest-rmse:0.371158+0.0209789\n",
      "[59]\ttrain-rmse:0.166423+0.00516871\ttest-rmse:0.371299+0.0211863\n",
      "[60]\ttrain-rmse:0.164374+0.00519022\ttest-rmse:0.371434+0.0205656\n",
      "[61]\ttrain-rmse:0.162958+0.00541282\ttest-rmse:0.371477+0.0204081\n",
      "[62]\ttrain-rmse:0.160383+0.00504164\ttest-rmse:0.371604+0.0203703\n",
      "[63]\ttrain-rmse:0.158868+0.00479861\ttest-rmse:0.371767+0.0200659\n",
      "[64]\ttrain-rmse:0.15696+0.00480634\ttest-rmse:0.37194+0.0198234\n",
      "[65]\ttrain-rmse:0.155247+0.00486978\ttest-rmse:0.372085+0.0200238\n",
      "[66]\ttrain-rmse:0.153155+0.00428591\ttest-rmse:0.372403+0.0195133\n",
      "[67]\ttrain-rmse:0.151037+0.00460279\ttest-rmse:0.372806+0.0196989\n",
      "[68]\ttrain-rmse:0.149659+0.00440001\ttest-rmse:0.372774+0.019717\n",
      "[69]\ttrain-rmse:0.148282+0.00416061\ttest-rmse:0.373091+0.0195167\n",
      "[70]\ttrain-rmse:0.147002+0.00381582\ttest-rmse:0.373269+0.0193129\n",
      "[71]\ttrain-rmse:0.14513+0.00362201\ttest-rmse:0.37348+0.0194826\n",
      "[72]\ttrain-rmse:0.143611+0.00363487\ttest-rmse:0.373317+0.0193079\n",
      "[73]\ttrain-rmse:0.14222+0.00350891\ttest-rmse:0.373479+0.0191912\n",
      "[74]\ttrain-rmse:0.140751+0.00310927\ttest-rmse:0.37351+0.0191769\n",
      "[75]\ttrain-rmse:0.139184+0.00284858\ttest-rmse:0.373917+0.0189115\n",
      "[76]\ttrain-rmse:0.137237+0.00301392\ttest-rmse:0.374387+0.0188495\n",
      "[77]\ttrain-rmse:0.13585+0.00313496\ttest-rmse:0.374545+0.0186619\n",
      "[78]\ttrain-rmse:0.133974+0.00309619\ttest-rmse:0.374766+0.01844\n",
      "[79]\ttrain-rmse:0.132981+0.0028996\ttest-rmse:0.374861+0.0182389\n",
      "[80]\ttrain-rmse:0.131401+0.00323652\ttest-rmse:0.375162+0.0184732\n",
      "[81]\ttrain-rmse:0.129644+0.00348503\ttest-rmse:0.375482+0.0183149\n",
      "[82]\ttrain-rmse:0.128004+0.00318848\ttest-rmse:0.37527+0.0182818\n",
      "[83]\ttrain-rmse:0.126408+0.00329304\ttest-rmse:0.375231+0.0183002\n",
      "[84]\ttrain-rmse:0.125257+0.00334406\ttest-rmse:0.37519+0.0182387\n",
      "[85]\ttrain-rmse:0.123874+0.00327275\ttest-rmse:0.375497+0.0180834\n",
      "[86]\ttrain-rmse:0.12241+0.00332788\ttest-rmse:0.375536+0.018213\n",
      "[87]\ttrain-rmse:0.121522+0.00315955\ttest-rmse:0.375624+0.0180442\n",
      "[88]\ttrain-rmse:0.120276+0.00301218\ttest-rmse:0.376024+0.0178177\n",
      "[89]\ttrain-rmse:0.118931+0.00307692\ttest-rmse:0.376165+0.0178582\n",
      "[90]\ttrain-rmse:0.117628+0.00326932\ttest-rmse:0.376381+0.0179226\n",
      "[91]\ttrain-rmse:0.116089+0.00276894\ttest-rmse:0.376545+0.0181189\n",
      "[92]\ttrain-rmse:0.114518+0.00297539\ttest-rmse:0.37673+0.0180257\n",
      "[93]\ttrain-rmse:0.11302+0.00290571\ttest-rmse:0.376863+0.0181091\n",
      "[94]\ttrain-rmse:0.111802+0.00294877\ttest-rmse:0.377127+0.0181136\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train_123n, z_train)\n",
    "dtest = xgb.DMatrix(X_test_123n)\n",
    "cvresult = xgb.cv(xgb_params1, dtrain, num_boost_round=1000, nfold=5,\n",
    "            metrics='rmse', early_stopping_rounds=50, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:1.66704\n",
      "[20]\ttrain-rmse:0.34711\n",
      "[40]\ttrain-rmse:0.221447\n",
      "[60]\ttrain-rmse:0.178533\n",
      "[80]\ttrain-rmse:0.146926\n",
      "[93]\ttrain-rmse:0.130928\n",
      "0.380832355499\n",
      "0.849884526559\n",
      "0.918032786885\n",
      "[[  4  48]\n",
      " [ 17 364]]\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 94\n",
    "model = xgb.train(xgb_params1, dtrain, num_boost_round = num_rounds, evals = [(dtrain, 'train')], verbose_eval = 20)\n",
    "z_pred1 = model.predict(dtest)\n",
    "print np.sqrt(mean_squared_error(z_test, z_pred1))\n",
    "predictions20 = [1 if x >=2 else 0 for x in z_pred1]\n",
    "check_result(y_test, predictions20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight={1:1.0, 0:3.8}, C=100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use feature set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_result(y_true, y_predict):\n",
    "    print accuracy_score(y_true, y_predict)\n",
    "    print f1_score(y_true, y_predict)\n",
    "    print confusion_matrix(y_true, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.906666666667\n",
      "0.951048951049\n",
      "[[  0  25]\n",
      " [  3 272]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_1n, y_train)\n",
    "predictions1 = model.predict(X_test_1n)\n",
    "check_result(y_test, predictions1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use feature set 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight={1:1.0, 0:2}, C=1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.854932301741\n",
      "[[  4  21]\n",
      " [ 54 221]]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_123n, y_train)\n",
    "predictions2 = model.predict(X_test_123n)\n",
    "check_result(y_test, predictions2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use feature set X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1009, 700]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-293-9e94e5739669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredictions3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcheck_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alin\\envs\\datascience\\lib\\site-packages\\sklearn\\linear_model\\logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[1;32m-> 1174\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1175\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alin\\envs\\datascience\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alin\\envs\\datascience\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 181\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1009, 700]"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight={1:1.0, 0:5}, C=1000.0)\n",
    "model.fit(X_train_4, y_train)\n",
    "predictions3 = model.predict(X_test_4)\n",
    "check_result(y_test, predictions3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use feature set X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833718244804\n",
      "0.908163265306\n",
      "[[  5  47]\n",
      " [ 25 356]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight={1:1.0, 0:1}, C=1000.0)\n",
    "model.fit(X_train_5, y_train)\n",
    "predictions3 = model.predict(X_test_5)\n",
    "check_result(y_test, predictions3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use feature set X14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817551963048\n",
      "0.897001303781\n",
      "[[ 10  42]\n",
      " [ 37 344]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight={1:1.0, 0:1}, C=1000.0)\n",
    "model.fit(X_train_14, y_train)\n",
    "predictions4 = model.predict(X_test_14)\n",
    "check_result(y_test, predictions4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use feature set X15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826789838337\n",
      "0.903969270166\n",
      "[[  5  47]\n",
      " [ 28 353]]\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(class_weight={1:1.0, 0:2.5}, C=1000.0)\n",
    "model.fit(X_train_15, y_train)\n",
    "predictions5 = model.predict(X_test_15)\n",
    "check_result(y_test, predictions5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use feature set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913333333333\n",
      "0.954703832753\n",
      "[[  0  25]\n",
      " [  1 274]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, class_weight = {1:1, 0:20})\n",
    "model.fit(X_train_1n, y_train)\n",
    "predictions6 = model.predict(X_test_1n)\n",
    "check_result(y_test, predictions6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.336921536452\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=500)\n",
    "model.fit(X_train_1n, z_train)\n",
    "pred = model.predict(X_test_1n)\n",
    "print np.sqrt(mean_squared_error(z_test, pred))\n",
    "\n",
    "#check_result(y_test, predictions6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849884526559\n",
      "0.917825537295\n",
      "[[  5  47]\n",
      " [ 18 363]]\n"
     ]
    }
   ],
   "source": [
    "predictions7 = np.array([1 if x >= 2 else 0 for x in pred])\n",
    "check_result(y_test, predictions7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use feature set 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879907621247\n",
      "0.936117936118\n",
      "[[  0  52]\n",
      " [  0 381]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, class_weight = {1:1, 0:10})\n",
    "model.fit(X_train_123n, y_train)\n",
    "predictions8 = model.predict(X_test_123n)\n",
    "check_result(y_test, predictions8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356999391385\n",
      "0.879907621247\n",
      "0.936117936118\n",
      "[[  0  52]\n",
      " [  0 381]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=500)\n",
    "model.fit(X_train_123n, z_train)\n",
    "pred = model.predict(X_test_123n)\n",
    "print np.sqrt(mean_squared_error(z_test, pred))\n",
    "predictions8 = np.array([1 if x >= 2 else 0 for x in pred])\n",
    "check_result(y_test, predictions8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use feature 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.877598152425\n",
      "0.934648581998\n",
      "[[  1  51]\n",
      " [  2 379]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, class_weight = {1:1, 0:10})\n",
    "model.fit(X_train_14, y_train)\n",
    "predictions9 = model.predict(X_test_14)\n",
    "check_result(y_test, predictions9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.365751545531\n",
      "0.877598152425\n",
      "0.934809348093\n",
      "[[  0  52]\n",
      " [  1 380]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=500)\n",
    "model.fit(X_train_14, z_train)\n",
    "pred = model.predict(X_test_14)\n",
    "print np.sqrt(mean_squared_error(z_test, pred))\n",
    "predictions10 = np.array([1 if x >= 2 else 0 for x in pred])\n",
    "check_result(y_test, predictions10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use feature 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875288683603\n",
      "0.933497536946\n",
      "[[  0  52]\n",
      " [  2 379]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, class_weight = {1:1, 0:10})\n",
    "model.fit(X_train_15, y_train)\n",
    "predictions11 = model.predict(X_test_15)\n",
    "check_result(y_test, predictions11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.357219999833\n",
      "0.863741339492\n",
      "0.926889714994\n",
      "[[  0  52]\n",
      " [  7 374]]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor(n_estimators=500)\n",
    "model.fit(X_train_15, z_train)\n",
    "pred = model.predict(X_test_15)\n",
    "print np.sqrt(mean_squared_error(z_test, pred))\n",
    "predictions12 = np.array([1 if x >= 2 else 0 for x in pred])\n",
    "check_result(y_test, predictions12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>2 mins 33 secs</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.10.5.4</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month and 1 day </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>alin</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.538 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.11 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------\n",
       "H2O cluster uptime:         2 mins 33 secs\n",
       "H2O cluster version:        3.10.5.4\n",
       "H2O cluster version age:    1 month and 1 day\n",
       "H2O cluster name:           alin\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.538 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "Python version:             2.7.11 final\n",
       "--------------------------  ------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use feature set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train0 = y_train.reshape(y_train.shape[0],1)\n",
    "z_train0 = z_train.reshape(z_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "0.875288683603\n",
      "0.933497536946\n",
      "[[  0  52]\n",
      " [  2 379]]\n"
     ]
    }
   ],
   "source": [
    "train = np.concatenate((X_train_1n, y_train0), axis = 1)\n",
    "train_hex = h2o.H2OFrame(train)\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x = range(train.shape[1]-1), y = train.shape[1]-1, training_frame=train_hex)\n",
    "\n",
    "test_hex = h2o.H2OFrame(X_test_1n)\n",
    "\n",
    "pred = gbm.predict(test_hex)\n",
    "\n",
    "pred1 = np.array([pred[i, 0] for i in range(y_test.shape[0])])\n",
    "\n",
    "predictions13 = [1 if x > 0.5 else 0 for x in pred1]\n",
    "check_result(y_test, predictions13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "0.363471146243\n",
      "0.866050808314\n",
      "0.928217821782\n",
      "[[  0  52]\n",
      " [  6 375]]\n"
     ]
    }
   ],
   "source": [
    "train = np.concatenate((X_train_1n, z_train0), axis = 1)\n",
    "train_hex = h2o.H2OFrame(train)\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x = range(train.shape[1]-1), y = train.shape[1]-1, training_frame=train_hex)\n",
    "\n",
    "test_hex = h2o.H2OFrame(X_test_1n)\n",
    "\n",
    "pred = gbm.predict(test_hex)\n",
    "\n",
    "pred1 = np.array([pred[i, 0] for i in range(y_test.shape[0])])\n",
    "print np.sqrt(mean_squared_error(z_test, pred1))\n",
    "predictions14 = [1 if x >= 2 else 0 for x in pred1]\n",
    "check_result(y_test, predictions14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use feature 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "0.879907621247\n",
      "0.936117936118\n",
      "[[  0  52]\n",
      " [  0 381]]\n"
     ]
    }
   ],
   "source": [
    "train = np.concatenate((X_train_123n, y_train0), axis = 1)\n",
    "train_hex = h2o.H2OFrame(train)\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x = range(train.shape[1]-1), y = train.shape[1]-1, training_frame=train_hex)\n",
    "\n",
    "test_hex = h2o.H2OFrame(X_test_123n)\n",
    "\n",
    "pred = gbm.predict(test_hex)\n",
    "\n",
    "pred1 = np.array([pred[i, 0] for i in range(y_test.shape[0])])\n",
    "\n",
    "predictions15 = [1 if x > 0.5 else 0 for x in pred1]\n",
    "check_result(y_test, predictions15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "0.376590871913\n",
      "0.854503464203\n",
      "0.92095357591\n",
      "[[  3  49]\n",
      " [ 14 367]]\n"
     ]
    }
   ],
   "source": [
    "train = np.concatenate((X_train_123n, z_train0), axis = 1)\n",
    "train_hex = h2o.H2OFrame(train)\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x = range(train.shape[1]-1), y = train.shape[1]-1, training_frame=train_hex)\n",
    "\n",
    "test_hex = h2o.H2OFrame(X_test_123n)\n",
    "\n",
    "pred = gbm.predict(test_hex)\n",
    "\n",
    "pred1 = np.array([pred[i, 0] for i in range(y_test.shape[0])])\n",
    "print np.sqrt(mean_squared_error(z_test, pred1))\n",
    "predictions16 = [1 if x >= 2 else 0 for x in pred1]\n",
    "check_result(y_test, predictions16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use feature 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_4a = X_train_4.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_4a = X_test_4.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "0.877598152425\n",
      "0.934809348093\n",
      "[[  0  52]\n",
      " [  1 380]]\n"
     ]
    }
   ],
   "source": [
    "train = np.concatenate((X_train_4a, y_train0), axis = 1)\n",
    "train_hex = h2o.H2OFrame(train)\n",
    "gbm = H2OGradientBoostingEstimator()\n",
    "gbm.train(x = range(train.shape[1]-1), y = train.shape[1]-1, training_frame=train_hex)\n",
    "\n",
    "test_hex = h2o.H2OFrame(X_test_4a)\n",
    "\n",
    "pred = gbm.predict(test_hex)\n",
    "\n",
    "pred1 = np.array([pred[i, 0] for i in range(y_test.shape[0])])\n",
    "\n",
    "predictions16 = [1 if x > 0.5 else 0 for x in pred1]\n",
    "check_result(y_test, predictions16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
