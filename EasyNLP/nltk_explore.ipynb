{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "r = urllib.urlopen('http://revistagalileu.globo.com/Ciencia/noticia/2017/08/foto-iconica-de-albert-einstein-e-leiloada-por-125-mil-dolares.html').read()\n",
    "soup = BeautifulSoup(r, \"html.parser\", from_encoding=\"utf-8\")\n",
    "print type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for div in soup.find_all('div'):\n",
    "    try:\n",
    "        if div['class'][0] == u'ctx_content':\n",
    "        #if div['class']:\n",
    "            target_div = div\n",
    "            #print div['class']\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print target_div.get_text()\n",
    "text = target_div.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1 = re.sub(r'\\([^\\(]+\\)', ' ', text)\n",
    "text2 = re.sub(r'Leia mais(.+)', '', text1)\n",
    "#def clean_text(text):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  A fotografia em que Albert Einstein mostra a língua foi leiloada pela bagatela de 125 mil dólares. O exemplar estava autografado pelo físico e foi comprado anonimamente, na casa de leilão Nate D. Sanders, nos Estados Unidos.A imagem foi feita em 1951, após o aniversário de 72 anos de Einstein, na Universidade de Princeton. O fotógrafo Arthur Sasse clicou o criador da Teoria da Relatividade na saída de uma festa.Um fato curioso é que o próprio Einstein adorava a imagem e até pediu cópias para usá-las como cartão de visita.A grande maioria das cópias da foto mostra só o rosto do vencedor do Nobel, mas a original também conta com o retrato do professor Frank Aydelotte e sua esposa, Marie Jeanette, ao lado de Einstein. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. length\n",
    "2. number of punctuation errors, number of different punctuation used\n",
    "3. grammatical errors, word useage errors, spelling errors, \n",
    "4. overly repetitious words\n",
    "5. Laten semantic analysis essay semantic similarity, vector length\n",
    "6. lexical sophistication (e.g. word maturity,word variety, and confusable words),\n",
    "7. grammar (e.g. n-gram attributes, grammatical errors, and grammar error types),\n",
    "8. mechanics (e.g. spelling, capitalization, and punctuation),\n",
    "9. style, organization, and development (e.g. sentence-sentence coherence, overall essay coherence, and topic development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Character count (char count)\n",
    "2.  Word count (word count)\n",
    "3. Number of exclamation marks\n",
    "4. Number of question marks\n",
    "5.  Number of “difficult” words (vocab). We obtained a list of 5000 words that frequently appears on the SAT2.\n",
    "6. Number of spelling mistakes (spelling). Spell checking was done using Enchant3.\n",
    "7.  Number of stopwords (stopwords). A list of 127-word stoplist was obtained from NLTK4.\n",
    "8. Part-of-speech n-grams. We considered unigrams, bigrams, and trigrams. Texts were\n",
    "tagged with the Stanford part-of-speech tagger (Toutanova et al, 2003). We only included\n",
    "feature instances that occurred in at least five different essays. Feature values were binarized:\n",
    "n-grams that were observed at least once in an essay have a corresponding feature\n",
    "value of 1, while unobserved n-grams have a value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. the number of enclise (and per token)\n",
    "2. the number of demonstrative pronouns (and per token)\n",
    "3. Grammar and style,  spelling error: https://github.com/giullianomorroni/JCorretorOrtografico\n",
    "4. sentences longer than 70 characters are bad. count number.\n",
    "5. flesh score\n",
    "6. number of different words\n",
    "7. average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 length:\n",
    "tokens = word_tokenize(text2)\n",
    "text = nltk.Text(tokens)\n",
    "text_length = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2 length without stop words\n",
    "tokens_f = [word for word in tokens if word not in stopwords]\n",
    "text_length_f = len(tokens_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Jersei', u'atinge', u'm\\xe9dia', u'de', u'Cr$', ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.mac_morpho.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Um', u'revivalismo', u'refrescante', u'O', ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floresta.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'Um', u'>N+art'), (u'revivalismo', u'H+n'), (u'refrescante', u'N<+adj')], [(u'O', u'>N+art'), (u'7_e_Meio', u'H+prop'), (u'\\xe9', u'P+v-fin'), (u'um', u'>N+art'), (u'ex-libris', u'H+n'), (u'de', u'H+prp'), (u'a', u'>N+art'), (u'noite', u'H+n'), (u'algarvia', u'N<+adj'), (u'.', u'.')], ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floresta.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'de', u'a', u'o', u'que', u'e', u'do', u'da', u'em', u'um', u'para']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stok = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "stemmer.stem('copiar')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division  # Python 2 users only\n",
    "import re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"C:/Users/alin/Documents/SelfStudy/MyLearning/EasyNLP/sample1.txt\", \"r\")\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw.decode('utf-8'))\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_text = nltk.sent_tokenize(raw.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "count vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lexical richness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7126436781609196"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'que', 7),\n",
       " (u'.', 7),\n",
       " (u'de', 6),\n",
       " (u',', 4),\n",
       " (u'um', 4),\n",
       " (u'n\\xe3o', 4),\n",
       " (u'nos', 3),\n",
       " (u'meus', 3),\n",
       " (u'com', 3),\n",
       " (u'onde', 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'completamente', u'demonstradas', u'descrev\\xea-lo', u'sentimentos']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = set(text)\n",
    "long_words = [w for w in V if len(w) > 10]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocation/bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sempre que\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " url = \"http://www.gutenberg.org/cache/epub/27364/pg27364.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = urlopen(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that n is the most common tag, so we can set up a default tagger that tags every word as a noun, and see how well it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger0 = nltk.DefaultTagger('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsents = floresta.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplify_tag(t):\n",
    "...     if \"+\" in t:\n",
    "...         return t[t.index(\"+\")+1:]\n",
    "...     else:\n",
    "...         return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " unigram tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " tagger1 = nltk.UnigramTagger(tsents, backoff=tagger0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger2 = nltk.BigramTagger(tsents, backoff=tagger1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged = tagger2.tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
