{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of algorithms and parameters in [H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit) repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Arbitrary order factorization machines](https://github.com/geffy/tffm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Cloud Computing](https://aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Python tSNE package](https://github.com/danielfrg/tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Effective use of pandas](https://tomaugspurger.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feature extraction from text with Sklearn](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Text feature extraction examples](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TextBlob](https://github.com/sloria/TextBlob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Using pretrained models in Keras](https://keras.io/applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/tutorials/image_retraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [How to select final model](http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Decision Trees: “Gini” vs. “Entropy” criteria](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Understanding ROC curves](http://www.navan.name/roc/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Learning to Rank using Gradient Descent](http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Learning to Rank Overview](https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tuning the hyper-parameters of an estimator sklearn](http://scikit-learn.org/stable/modules/grid_search.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optimizing hyperparameters with hyperopt](http://fastml.com/optimizing-hyperparams-with-hyperopt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Guide to Parameter Tuning in Gradient Boosting (GBM)](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Far0n's framework for Kaggle competitions kaggletils](https://github.com/Far0n/kaggletils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Overview of Matrix Decomposition methods (sklearn)](Overview of Matrix Decomposition methods (sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Multicore t-SNE implementation](Multicore t-SNE implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Comparison of Manifold Learning methods (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [How to Use t-SNE Effectively (distill.pub blog)](https://distill.pub/2016/misread-tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [tSNE homepage (Laurens van der Maaten)](https://lvdmaaten.github.io/tsne/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Example: tSNE with different perplexities (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Facebook Research's paper about extracting categorical features from trees](https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Example: Feature transformations with ensembles of trees (sklearn)](http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Kaggle ensembling guide at MLWave.com (overview of approaches)](https://mlwave.com/kaggle-ensembling-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [StackNet](https://github.com/kaz-Anova/StackNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Heamy — a set of useful tools for competitive data science (including ensembling)](https://github.com/rushter/heamy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "1. Do basic preprocessing and convert csv/txt files into hdf5(for panda dataframes)/npy(for numpy arrays) for faster loading\n",
    "2. By default pandas data is stored in 64-bit arrays, most times can be safely downcast  to 32-bit.\n",
    "3. Large datasets can be processed in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features:\n",
    "\n",
    "#### Transformation\n",
    "1. Do scaling for non-tree based models.\n",
    "2. For outliers, can try clipping by value or by percentage, also know as winsorization.\n",
    "3. Rank transformation, scipy.stats.rankdata()\n",
    "4. np.log(1+x), np.sqrt(x+2/3) etc., useful for non-tree model esp. neural net.\n",
    "\n",
    "#### Feature generation\n",
    "1. ratio, e.g. price per square foot.\n",
    "2. $+-*/$ features can be helpful. For example, even GBM has difficulty approximating these simple operations.\n",
    "3. take fractional part, e.g. $2.49 --> 0.49$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Categorical Features\n",
    "\n",
    "#### Transformation\n",
    "\n",
    "1. Label Encoding: sklearn.preprocessing.LabelEncoder or pandas.factorize, mainly for tree-based model\n",
    "2. Frequency Encoding: map values to their frequencies, mainly for tree-based model\n",
    "3. 1-hot Encoding: often used for non-tree based model\n",
    "4. May consider replacing levels by the mean of certain numerical features\n",
    "\n",
    "#### Feature generation (before imputation of missing data)\n",
    "\n",
    "1. Add feature interaction: for example PClass + Gender --> 1Male, 2Female etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Mean encoding (given a categorical variable, for each \n",
    "Goods - number of 1 in a level, Bads - number of 0 in a level\n",
    "1. Likelihood = mean(target)\n",
    "2. Weight of evidence = ln(Goods/Bads) * 100\n",
    "3. Count = Goods\n",
    "4. Diff = Goods - Bads\n",
    "\n",
    "Example:\n",
    "means = X_tr.groupby(col).target.mean()\n",
    "\n",
    "train_new[col+'_mean_target'] = train_new[col].map(means)\n",
    "\n",
    "var_new[col+'_mean_target'] = var_new[col].map(means)\n",
    "\n",
    "### Mean encoding can cause overfitting, needs regularization\n",
    "####  CV loop inside training data (enough data, 4 to 5 folds)\n",
    "\n",
    "y_tr = df_tr['target'].values\n",
    "\n",
    "skf = StratifiedKFold(y_tr, 5, shuffle=True, random_state=42)\n",
    "\n",
    "for tr_ind, val_ind in skf:\n",
    "\n",
    "    X_tr, X_val = df_tr.iloc[tr_ind], df_tr.iloc[val_ind]\n",
    "    \n",
    "    for col in cols:\n",
    "    \n",
    "        means = X_val[col].map(X_tr.groupby(col).target.mean())\n",
    "        \n",
    "        X_val[col + '_mean_target'] =means\n",
    "   \n",
    "   train_new.iloc[val_ind] = X_val\n",
    "\n",
    "prior = df_tr['target'].mean()\n",
    "\n",
    "train_new.fillna(prior, inplace=True)\n",
    "\n",
    "####  Smoothing:\n",
    "$\n",
    "\\frac{mean(target) * nrows + globalmean * \\alpha}{nrows + alpha}\n",
    "$\n",
    "\n",
    "#### Adding random noise (hard to work)\n",
    "\n",
    "####  Sorting and calculating expanding mean (used in CatBoost, check it out)\n",
    "\n",
    "cumsum = df_tr.groupby(col)['target'].cumsum() - df_tr['target']\n",
    "\n",
    "cumcnt = df_tr.goupby(col).cumcount()\n",
    "\n",
    "train_new['col + '_mean_target'] = cumsum/cumcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DateTime Features\n",
    "\n",
    "1. Periodicity: day number in week, month, season, year. Second, minute, hour etc.\n",
    "2. Time-since (a. a fixed date such as 1/1/2000, b. e.g. last holiday etc, to next holiday)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates\n",
    "\n",
    "1. Distance to the nearest interesting place\n",
    "2. Calculate aggregrate statistics for objects surrounding area\n",
    "3. Do clustering first, then distance to the center.\n",
    "4. if train decision tree from coordinates, can add slightly rotated coordinates as new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction from Text\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "1. Preprocessing: lowercase, stemming, lemmetization, stopwords\n",
    "2. Ngram\n",
    "3. Postprocessing: TFIDF\n",
    "\n",
    "#### Word2vec, Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query comparision features\n",
    "1. number of matching words\n",
    "2. cosine distance between tf-idf representations\n",
    "3. distance between average word2vec vectors\n",
    "4. Levenshtein distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics and distance based features\n",
    "1. E.g.,  give data with columns: uid, page_id, ad_price, ad_position, can add max/min price per user/page and/or min_price_position per user/page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3a23e4a58cd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'page_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ad_price'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'max_price'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'min_price'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'page_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'min_price'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'page_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "gb = df.groupby(['user_id', 'page_id'], as_index=False).agg({'ad_price':{'max_price':np.max, 'min_price': np.min}})\n",
    "gb.columns = ['user_id', 'page_id', 'min_price', 'max_price']\n",
    "df = pd.merge(df, gb, how='left', on=['user_id', 'page_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also try \n",
    "1. How many pages user visited\n",
    "2. Standard deviation of prices\n",
    "3. Most visited page\n",
    "4. many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Can also use neighbors:\n",
    "    - Explicit group is not needed\n",
    "    - More flexible\n",
    "    - Much harder to implement\n",
    "    \n",
    "    \n",
    "    Example:\n",
    "    - Number of houses in 500m, 1000m etc.\n",
    "    - Average price per square meter in 500m, 1000m etc.\n",
    "    - Number of schools/supermarkets/parking lots in 500m, 1000m etc.\n",
    "    - Distance to the closest subway station\n",
    "    \n",
    "    Concrete example:\n",
    "    - mean encoding all variables\n",
    "    - for every point, find 2000 nearest neighbors using Bray-Curtis metric\n",
    "    - Calculate various features:\n",
    "    - eg: mean target of nearest 5, 10, 15, 500, 2000 neighbors\n",
    "    - eg: mean distance to the 10 closest neighbors\n",
    "    - eg: mean distance to the 10 closest neighbors with target 0\n",
    "    - eg: mean distance to the 10 closest negibhors with target 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization for Feature Extraction\n",
    "1. Can be applied to only some columns\n",
    "2. Can provide additional diversity, good for ensembles\n",
    "3. It's a lossy transformation, usually choose 5 - 100 latent factors\n",
    "4. check out sklearn: SVD, PCA, truncated SVD (works with sparse matrices), Non-negative matrix factorization (good for oucnts like data) \n",
    "5. can often be used to log(x), log(x+1) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Interaction\n",
    "Eg. Feature1, Feature2 ---> Feature1_Feature2 (can use concatination or something like sum, division and other functional operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features from Decision Tree\n",
    "Mark each leaf into a binary feature\n",
    "\n",
    "1. sklearn: tree_model.apply()\n",
    "2. xgboost: booster.predict(pred_leaf = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t_SNE method for feature extraction\n",
    "1. sklearn\n",
    "2. tsne package is better\n",
    "3. Perplexity parameter 5 to 100 often"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot variable importance\n",
    "plt.plot(rf.feature_importance)\n",
    "\n",
    "plt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);\n",
    "\n",
    "#### show progress bar\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "#### Single Variable\n",
    "1. histogram: plt.hist(x)\n",
    "2. plt.plot(x, '.')\n",
    "3. scatter plot with label as color: plt.scatter(range(len(x)), x, c=y)\n",
    "\n",
    "#### Feature Pairs\n",
    "1. plt.scatter(x1, x2)\n",
    "2. pd.scatter_matrix(df)\n",
    "3. feature groups, e.g. plot sorted mean value: df.mean().sort_values().plot(stype='.')\n",
    "\n",
    "#### Tools\n",
    "1. Seaborn\n",
    "2. Plotty\n",
    "3. Bokeh\n",
    "4. ggplot\n",
    "5. networkx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Checking\n",
    "\n",
    "#### Duplicated and Constant Features\n",
    "1. train.nunique(axis=1) = 1\n",
    "2. train.T.drop_duplicates()\n",
    "3. Categorical duplicated features: F1, F2 just have different levels, e.g.,  F1: A B C, F2: C D E.\n",
    "\n",
    " for f in categorical_features:\n",
    " \n",
    "     train[f] = train[f].factorize()\n",
    " \n",
    " train.T.drop_duplicates()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicated Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if dataset is shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove constant features\n",
    "\n",
    "`dropna = False` makes nunique treat NaNs as a distinct value\n",
    "\n",
    "feats_counts = train.nunique(dropna = False)\n",
    "\n",
    "feats_counts.sort_values()[:10]\n",
    "\n",
    "constant_features = feats_counts.loc[feats_counts==1].index.tolist()\n",
    "\n",
    "print (constant_features)\n",
    "\n",
    "\n",
    "traintest.drop(constant_features,axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated features\n",
    "\n",
    "Fills NaNs with something we can find later if needed.\n",
    "\n",
    "##### traintest.fillna('NaN', inplace=True)\n",
    "\n",
    " encode each feature:\n",
    " \n",
    " for col in tqdm_notebook(traintest.columns):\n",
    "    train_enc[col] = train[col].factorize()[0]\n",
    "    \n",
    "    \n",
    " up_cols = {}\n",
    "\n",
    "for i, c1 in enumerate(tqdm_notebook(train_enc.columns)):\n",
    "\n",
    "    for c2 in train_enc.columns[i + 1:]:\n",
    "\n",
    "        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):\n",
    "        \n",
    "            dup_cols[c2] = c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get cat columns and num columns\n",
    "cat_cols = list(train.select_dtypes(include=['object']).columns)\n",
    "\n",
    "num_cols = list(train.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value:\n",
    "check the number of Nans for each row, maybe a good feature.\n",
    "\n",
    "train.isnull().sum(axis=1).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimize metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability calibration\n",
    "1. Platt scaling: just fit Logistic Regression to your predictions (like stacking)\n",
    "2. Isotonic regression: Just fit Isotonic regression to your predictions (like stacking)\n",
    "3. Stacking: Just fit XGboost or neural net to your predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypter parameters tuning\n",
    "### Tools:\n",
    "1. Hypteropt\n",
    "2. scikit-optimize\n",
    "3. Spearmint\n",
    "4.  GpyOpt\n",
    "\n",
    "### Color coding parameters\n",
    "1. red: used to constrain the model: increasing it impedes fitting, but reduces overfitting; decreases to allow model fit easier\n",
    "2. green:  opposite to red\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree based models\n",
    "\n",
    "#### GBDT: xgboost, lightgbm, catboost\n",
    "\n",
    "\n",
    "#### RandomForest, ExtraTrees: scikit-learn\n",
    "\n",
    "#### Othres: RGF(baidu/fast_rgf)  regularized greedy forest  (slow, can be used for small datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost and lightGBM\n",
    "\n",
    "1. max_depth(xgb, lgb), num_leaves(lgb):  if increasing it cannot lead to overfitting, then there might be lots of intersactions, stop tunning, find new features.  Start with 7.\n",
    "2. subsample(xgb), bagging_fraction(lgb)\n",
    "3. colsample_bytree, col_sample_bylevel(xgb),  feature_faction(lgb)\n",
    "4. min_child_weight (xgb), min_data_in_leaf(lgb). Increasing it will causes the model to be more conservative. __Important parameters, try value 0, 5, 15, 300 __\n",
    "5. eta(xgb), learning_rate(lgb), num_round(xgb), num_iterations(xgb): fix eta to be small such as 0.1 or 0.001, then find how many rounds it will take to overfit.  After finding the number of steps using early stopping, there is a __trick, multiple the rounds by alpha, then divide the learning_rate by alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest and Extra Trees\n",
    "1. Number of trees: start with 10 to see how fast it trains, then set to a big number and plot the curve of validationt error v.s. number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Nets (FNN)\n",
    "1. Number of neurons per layers\n",
    "2. Number of layers\n",
    "3. optimization methods: advanced methods such as Adam etc. are faster but often lead to overfitting, while SGD + momentum is slower but often generalizer better.\n",
    "4. large batch size often leads to overfitting, 32 or 64 is better.\n",
    "5. learning rate, usually starts with 0.1, then reduce it.\n",
    "6. rule of thumb: if increases the batch size by a factor of $\\alpha$, can also increase learning rate by the same factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear models\n",
    "1. Scikit Learn\n",
    "      - SVC/SVR: sklearn wraps libLinear and libSVM, compile for multicore support \n",
    "      - LogisticRegression / LinearRegression + regularizers\n",
    "      - SGDCClassifier / SGDRegressor\n",
    "2. For datasets that don't fit in the memory, we can use Vowpal Wabbit:\n",
    "   - FTRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fb59c76fbc3d>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-fb59c76fbc3d>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    X_f['item_target_enc'] = means`\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "bags = 10\n",
    "seed = 1\n",
    "\n",
    "bagged_prediction = np.zero(test.shape[0])\n",
    "\n",
    "for n in range(0, bags):\n",
    "    model.set_params(random_state = seed + n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    bagged_prediction += preds\n",
    "bagged_prediction /= bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "1. Adaboost in sklearn is very good\n",
    "2. LogitBoost in Weka\n",
    "\n",
    "\n",
    "- XGboost\n",
    "- LightGBM\n",
    "- H2o's GBM (can handel categorical variaible directly\n",
    "- Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st level tips\n",
    "* Diversity based on algorithms:\n",
    "    + 2-3 gradient boosted trees (lightgbm, xgboost, h2o, catboost)\n",
    "    + 2-3 neural nets (keras, pytorch) maybe one a bit deep with 3 hidden layers, one with 2 hidden layers, one with 1 hidden layer.\n",
    "    + 1-2 extra tree / random forest\n",
    "    + 1-2 linear models such as logistic regression, ridge regression, linear svm etc.\n",
    "    + 1-2 knn\n",
    "    + 1 Factorization machine (libfmz)\n",
    "    + if data permit, 1 svm with nonlinear kernel\n",
    "* Diversity based on input data:\n",
    "    + Categorical features: try 1-hot, label-encoding, target-encoding\n",
    "    + Numerical features: outliers, binning, derivatives, percentiles, scaling, \n",
    "    + Interactions, col1 */+- col2, unsuervised (such as kmean etc), groupby \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsequent level tips:\n",
    "   * Simpler (or shallower) Algorithm\n",
    "       + gradient boosted trees with small depth (2, 3)\n",
    "       + Linear models with high regularization\n",
    "       + Extra trees\n",
    "       + Shallow network (1 hidden layer)\n",
    "       + Knn with BrayCurtis Distance\n",
    "       + Brute force search for best linear weights based on cv\n",
    "   * Feature engineering:\n",
    "       + pairwise difference between meta features\n",
    "       + row-wise statics such as averages or stds\n",
    "       + standard feature selection techniques\n",
    "       * For every 7.5 models in previous level we add 1 in the mets\n",
    "   * Be mindful of target leekage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Software for stacking\n",
    "\n",
    "* StackNet\n",
    "* Stacked ensembles from H2O\n",
    "* Xcessiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips about StackNet\n",
    "\n",
    "* It supports many prominent tools (xgboost, lightgbm, h2o, keras)\n",
    "* Can run classifiers in regression and vice vera\n",
    "* [parameters section for other tools](https://github.com/kaz-Anova/StackNet/blob/master/parameters/PARAMETERS.MD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking validation\n",
    "\n",
    "There are a number of ways to validate second level models (meta-models). In this reading material you will find a description for the most popular ones. If not specified, we assume that the data does not have a time component. We also assume we already validated and fixed hyperparameters for the first level models (models).\n",
    "\n",
    "\n",
    "a) Simple holdout scheme\n",
    "\n",
    "1. Split train data into three parts: partA and partB and partC.\n",
    "2. Fit N diverse models on partA, predict for partB, partC, test_data getting meta-features partB_meta, partC_meta and test_meta respectively.\n",
    "3. Fit a metamodel to a partB_meta while validating its hyperparameters on partC_meta.\n",
    "4. When the metamodel is validated, fit it to [partB_meta, partC_meta] and predict for test_meta.\n",
    "\n",
    "\n",
    "b) Meta holdout scheme with OOF meta-features\n",
    "\n",
    "1. Split train data into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in train_data we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them train_meta.\n",
    "2. Fit models to whole train data and predict for test data. Let's call these features test_meta.\n",
    "3. Split train_meta into two parts: train_metaA and train_metaB. Fit a meta-model to train_metaA while validating its hyperparameters on train_metaB.\n",
    "4. When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
    "\n",
    "\n",
    "c) Meta KFold scheme with OOF meta-features\n",
    "\n",
    "1. Obtain OOF predictions train_meta and test metafeatures test_meta using b.1 and b.2.\n",
    "2. Use KFold scheme on train_meta to validate hyperparameters for meta-model. A common practice to fix seed for this KFold to be the same as seed for KFold used to get OOF predictions.\n",
    "3. When the meta-model is validated, fit it to train_meta and predict for test_meta.\n",
    "\n",
    "\n",
    "d) Holdout scheme with OOF meta-features\n",
    "\n",
    "1. Split train data into two parts: partA and partB.\n",
    "2. Split partA into K folds. Iterate though each fold: retrain N diverse models on all folds except current fold, predict for the current fold. After this step for each object in partA we will have N meta-features (also known as out-of-fold predictions, OOF). Let's call them partA_meta.\n",
    "3. Fit models to whole partA and predict for partB and test_data, getting partB_meta and test_meta respectively.\n",
    "4. Fit a meta-model to a partA_meta, using partB_meta to validate its hyperparameters.\n",
    "5. When the meta-model is validated basically do 2. and 3. without dividing train_data into parts and then train a meta-model. That is, first get out-of-fold predictions train_meta for the train_data using models. Then train models on train_data, predict for test_data, getting test_meta. Train meta-model on the train_meta and predict for test_meta.\n",
    "\n",
    "\n",
    "e) KFold scheme with OOF meta-features\n",
    "\n",
    "1. To validate the model we basically do d.1 -- d.4 but we divide train data into parts partA and partB M times using KFold strategy with M folds.\n",
    "2. When the meta-model is validated do d.5.\n",
    "\n",
    "### Validation in presence of time component\n",
    "\n",
    "\n",
    "f) KFold scheme in time series\n",
    "\n",
    "In time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.\n",
    "\n",
    "1. Split the train data into chunks of duration T. Select first M chunks.\n",
    "2. Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
    "3. Now we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features.\n",
    "\n",
    "\n",
    "g) KFold scheme in time series with limited amount of data\n",
    "\n",
    "We may often encounter a situation, where scheme f) is not applicable, especially with limited amount of data. For example, when we have only years 2014, 2015, 2016 in train and we need to predict for a whole year 2017 in test. In such cases scheme c) could be of help, but with one constraint: KFold split should be done with the respect to the time component. For example, in case of data with several years we would treat each year as a fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestion1.\n",
    "\n",
    "A good exercise is to reproduce previous_value_benchmark. As the name suggest - in this benchmark for the each shop/item pair our predictions are just monthly sales from the previous month, i.e. October 2015.\n",
    "\n",
    "The most important step at reproducing this score is correctly aggregating daily data and constructing monthly sales data frame. You need to get lagged values, fill NaNs with zeros and clip the values into [0,20] range. If you do it correctly, you'll get precisely 1.16777 on the public leaderboard.\n",
    "\n",
    "Generating features like this is a necessary basis for more complex models. Also, if you decide to fit some model, don't forget to clip the target into [0,20] range, it makes a big difference.\n",
    "\n",
    "### Suggestion2.\n",
    "\n",
    "You can get a rather good score after creating some lag-based features like in advice from previous week and feeding them into gradient boosted trees model.\n",
    "\n",
    "Apart from item/shop pair lags you can try adding lagged values of total shop or total item sales (which are essentially mean-encodings). All of that is going to add some new information.\n",
    "\n",
    "### Suggestion3.\n",
    "\n",
    "If you successfully made use of previous advises, it's time to move forward and incorporate some new knowledge from week 4. Here are several things you can do:\n",
    "\n",
    "1. Try to carefully tune hyper parameters of your models, maybe there is a better set of parameters for your model out there. But don't spend too much time on it.\n",
    "2. Try ensembling. Start with simple averaging of linear model and gradient boosted trees like in programming assignment notebook. And then try to use stacking.\n",
    "3. Explore new features! There is a lot of useful information in the data: text descriptions, item categories, seasonal trends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
