{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of algorithms and parameters in [H2O documentation](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit) repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Arbitrary order factorization machines](https://github.com/geffy/tffm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AWS Cloud Computing](https://aws.amazon.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Python tSNE package](https://github.com/danielfrg/tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries to work with sparse CTR-like data: [LibFM](http://www.libfm.org/), [LibFFM](https://www.csie.ntu.edu.tw/~cjlin/libffm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another tree-based method: RGF ([implemetation](https://github.com/baidu/fast_rgf), [paper](https://arxiv.org/pdf/1109.0887.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Effective use of pandas](https://tomaugspurger.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [Feature Scaling and the effect of standardization for machine learning algorithms](http://sebastianraschka.com/Articles/2014_about_feature_scaling.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Discussion of feature engineering on Quora](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Feature extraction from text with Sklearn](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Text feature extraction examples](https://andhint.github.io/machine-learning/nlp/Feature-Extraction-From-Text/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial to Word2vec](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tutorial to word2vec usage](https://rare-technologies.com/word2vec-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TextBlob](https://github.com/sloria/TextBlob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Using pretrained models in Keras](https://keras.io/applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Image classification with a pre-trained deep neural network](https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [How to Retrain Inception's Final Layer for New Categories in Tensorflow](https://www.tensorflow.org/tutorials/image_retraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Fine-tuning Deep Learning Models in Keras](https://flyyufelix.github.io/2016/10/08/fine-tuning-in-keras-part2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [How to select final model](http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Decision Trees: “Gini” vs. “Entropy” criteria](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Understanding ROC curves](http://www.navan.name/roc/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Learning to Rank using Gradient Descent](http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [RankLib](https://sourceforge.net/p/lemur/wiki/RankLib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Learning to Rank Overview](https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Features:\n",
    "\n",
    "#### Transformation\n",
    "1. Do scaling for non-tree based models.\n",
    "2. For outliers, can try clipping by value or by percentage, also know as winsorization.\n",
    "3. Rank transformation, scipy.stats.rankdata()\n",
    "4. np.log(1+x), np.sqrt(x+2/3) etc., useful for non-tree model esp. neural net.\n",
    "\n",
    "#### Feature generation\n",
    "1. ratio, e.g. price per square foot.\n",
    "2. $+-*/$ features can be helpful. For example, even GBM has difficulty approximating these simple operations.\n",
    "3. take fractional part, e.g. $2.49 --> 0.49$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Categorical Features\n",
    "\n",
    "#### Transformation\n",
    "\n",
    "1. Label Encoding: sklearn.preprocessing.LabelEncoder or pandas.factorize, mainly for tree-based model\n",
    "2. Frequency Encoding: map values to their frequencies, mainly for tree-based model\n",
    "3. 1-hot Encoding: often used for non-tree based model\n",
    "4. May consider replacing levels by the mean of certain numerical features\n",
    "\n",
    "#### Feature generation (before imputation of missing data)\n",
    "\n",
    "1. Add feature interaction: for example PClass + Gender --> 1Male, 2Female etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Mean encoding (given a categorical variable, for each \n",
    "Goods - number of 1 in a level, Bads - number of 0 in a level\n",
    "1. Likelihood = mean(target)\n",
    "2. Weight of evidence = ln(Goods/Bads) * 100\n",
    "3. Count = Goods\n",
    "4. Diff = Goods - Bads\n",
    "\n",
    "Example:\n",
    "means = X_tr.groupby(col).target.mean()\n",
    "\n",
    "train_new[col+'_mean_target'] = train_new[col].map(means)\n",
    "\n",
    "var_new[col+'_mean_target'] = var_new[col].map(means)\n",
    "\n",
    "### Mean encoding can cause overfitting, needs regularization\n",
    "####  CV loop inside training data (enough data, 4 to 5 folds)\n",
    "\n",
    "y_tr = df_tr['target'].values\n",
    "\n",
    "skf = StratifiedKFold(y_tr, 5, shuffle=True, random_state=42)\n",
    "\n",
    "for tr_ind, val_ind in skf:\n",
    "\n",
    "    X_tr, X_val = df_tr.iloc[tr_ind], df_tr.iloc[val_ind]\n",
    "    \n",
    "    for col in cols:\n",
    "    \n",
    "        means = X_val[col].map(X_tr.groupby(col).target.mean())\n",
    "        \n",
    "        X_val[col + '_mean_target'] =means\n",
    "   \n",
    "   train_new.iloc[val_ind] = X_val\n",
    "\n",
    "prior = df_tr['target'].mean()\n",
    "\n",
    "train_new.fillna(prior, inplace=True)\n",
    "\n",
    "####  Smoothing:\n",
    "$\n",
    "\\frac{mean(target) * nrows + globalmean * \\alpha}{nrows + alpha}\n",
    "$\n",
    "\n",
    "#### Adding random noise (hard to work)\n",
    "\n",
    "####  Sorting and calculating expanding mean (used in CatBoost, check it out)\n",
    "\n",
    "cumsum = df_tr.groupby(col)['target'].cumsum() - df_tr['target']\n",
    "\n",
    "cumcnt = df_tr.goupby(col).cumcount()\n",
    "\n",
    "train_new['col + '_mean_target'] = cumsum/cumcnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DateTime Features\n",
    "\n",
    "1. Periodicity: day number in week, month, season, year. Second, minute, hour etc.\n",
    "2. Time-since (a. a fixed date such as 1/1/2000, b. e.g. last holiday etc, to next holiday)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates\n",
    "\n",
    "1. Distance to the nearest interesting place\n",
    "2. Calculate aggregrate statistics for objects surrounding area\n",
    "3. Do clustering first, then distance to the center.\n",
    "4. if train decision tree from coordinates, can add slightly rotated coordinates as new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction from Text\n",
    "\n",
    "#### Bag of words\n",
    "\n",
    "1. Preprocessing: lowercase, stemming, lemmetization, stopwords\n",
    "2. Ngram\n",
    "3. Postprocessing: TFIDF\n",
    "\n",
    "#### Word2vec, Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot variable importance\n",
    "plt.plot(rf.feature_importance)\n",
    "\n",
    "plt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);\n",
    "\n",
    "#### show progress bar\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "#### Single Variable\n",
    "1. histogram: plt.hist(x)\n",
    "2. plt.plot(x, '.')\n",
    "3. scatter plot with label as color: plt.scatter(range(len(x)), x, c=y)\n",
    "\n",
    "#### Feature Pairs\n",
    "1. plt.scatter(x1, x2)\n",
    "2. pd.scatter_matrix(df)\n",
    "3. feature groups, e.g. plot sorted mean value: df.mean().sort_values().plot(stype='.')\n",
    "\n",
    "#### Tools\n",
    "1. Seaborn\n",
    "2. Plotty\n",
    "3. Bokeh\n",
    "4. ggplot\n",
    "5. networkx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Checking\n",
    "\n",
    "#### Duplicated and Constant Features\n",
    "1. train.nunique(axis=1) = 1\n",
    "2. train.T.drop_duplicates()\n",
    "3. Categorical duplicated features: F1, F2 just have different levels, e.g.,  F1: A B C, F2: C D E.\n",
    "\n",
    " for f in categorical_features:\n",
    " \n",
    "     train[f] = train[f].factorize()\n",
    " \n",
    " train.T.drop_duplicates()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicated Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if dataset is shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove constant features\n",
    "\n",
    "`dropna = False` makes nunique treat NaNs as a distinct value\n",
    "\n",
    "feats_counts = train.nunique(dropna = False)\n",
    "\n",
    "feats_counts.sort_values()[:10]\n",
    "\n",
    "constant_features = feats_counts.loc[feats_counts==1].index.tolist()\n",
    "\n",
    "print (constant_features)\n",
    "\n",
    "\n",
    "traintest.drop(constant_features,axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated features\n",
    "\n",
    "Fills NaNs with something we can find later if needed.\n",
    "\n",
    "##### traintest.fillna('NaN', inplace=True)\n",
    "\n",
    " encode each feature:\n",
    " \n",
    " for col in tqdm_notebook(traintest.columns):\n",
    "    train_enc[col] = train[col].factorize()[0]\n",
    "    \n",
    "    \n",
    " up_cols = {}\n",
    "\n",
    "for i, c1 in enumerate(tqdm_notebook(train_enc.columns)):\n",
    "\n",
    "    for c2 in train_enc.columns[i + 1:]:\n",
    "\n",
    "        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):\n",
    "        \n",
    "            dup_cols[c2] = c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get cat columns and num columns\n",
    "cat_cols = list(train.select_dtypes(include=['object']).columns)\n",
    "\n",
    "num_cols = list(train.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value:\n",
    "check the number of Nans for each row, maybe a good feature.\n",
    "\n",
    "train.isnull().sum(axis=1).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimize metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability calibration\n",
    "1. Platt scaling: just fit Logistic Regression to your predictions (like stacking)\n",
    "2. Isotonic regression: Just fit Isotonic regression to your predictions (like stacking)\n",
    "3. Stacking: Just fit XGboost or neural net to your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for r_ind, f_ind in kf.split(all_data):\n",
    "    print('f_ind', len(f_ind), 'r_ind', len(r_ind))\n",
    "    X_r, X_f = all_data.iloc[r_ind], all_data.iloc[f_ind]\n",
    "    means = X_f['item_id'].map(X_r.groupby('item_id').target.mean())\n",
    "    X_f['item_target_enc'] = means`\n",
    "    result.append(X_f)\n",
    "#all_data['item_target_enc'].fillna(0.3343, inplace=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['item_target_enc'] = all_data.groupby('item_id')['target'].transform('mean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
