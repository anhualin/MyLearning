{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea:\n",
    "Given $k$, build a model to predict the number of visitors after k days using the following features:\n",
    "1. (holidayflag, day_of_week, is_closed, #visitors) for the past n weeks.\n",
    "2. store_id, gentre, area\n",
    "\n",
    "Only do label encoding to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Linux':\n",
    "    data_dir = '/home/alin/Data/Recruit_Holding'\n",
    "else:\n",
    "    data_dir = 'C:/Users/alin/Documents/Data/Recruit_Holding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load previous dump\n"
     ]
    }
   ],
   "source": [
    "DFS_dump = data_dir + '/DFS.p'\n",
    "if Path(DFS_dump).is_file():\n",
    "    print('load previous dump')\n",
    "    DFS = pickle.load(open(DFS_dump, 'rb'))\n",
    "    air_reserve = DFS['air_reserve']\n",
    "    air_reserve_day = DFS['air_reserve_day']\n",
    "    hpg_reserve = DFS['hpg_reserve']\n",
    "    hpg_reserve_day = DFS['hpg_reserve_day']\n",
    "    air_visit_hist = DFS['air_visit_hist']\n",
    "    date_info = DFS['date_info']\n",
    "    test = DFS['test']\n",
    "    air_store_info = DFS['air_store_info']\n",
    "    hpg_store_info = DFS['hpg_store_info']\n",
    "    store_id_relation = DFS['store_id_relation']\n",
    "    test = DFS['test']\n",
    "else:\n",
    "    print('run EDA1 first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the training and testing datasets before label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: add dates when a store is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(k = 3):\n",
    "    '''\n",
    "    Keep the last k weeks of air_vist_hist, then for any store missing on any day,  create the corresponding \n",
    "    row with expacted valud 0\n",
    "    '''\n",
    "    last_train_day = max(air_visit_hist.day_ind)\n",
    "    first_train_day = last_train_day - k * 7 + 1\n",
    "    \n",
    "    #filter into desire time frame\n",
    "    hist1 = air_visit_hist[(air_visit_hist.day_ind >= first_train_day) & (air_visit_hist.day_ind <= last_train_day)].copy()\n",
    "    all_stores = hist1.air_store_id.unique()\n",
    "    all_days = [i for i in range(first_train_day, last_train_day+1)]\n",
    "    \n",
    "    #create store x day grid\n",
    "    grid = np.array(list(product(*[all_stores, all_days])))\n",
    "    grid = pd.DataFrame(grid, columns=['air_store_id', 'day_ind_str' ])\n",
    "    grid['day_ind'] = grid.apply(lambda r: int(r['day_ind_str']), axis=1)\n",
    "    grid.drop('day_ind_str', axis=1, inplace=True)\n",
    "    \n",
    "    # add visit information \n",
    "    all_data = grid.merge(hist1, how='left', on=['air_store_id', 'day_ind'])\n",
    "    \n",
    "    # add date type information\n",
    "    all_data = all_data.merge(date_info, on='day_ind', suffixes=['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # add store information\n",
    "    all_data = all_data.merge(air_store_info, on = 'air_store_id', suffixes = ['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l'] + ['calendar_date', 'date', 'latitude', 'longitude', 'hpg_store_id']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # for those dates on which the visit informaiton of a store is missing, assume that it was closed abd with visit number 0\n",
    "    all_data['closed'] = all_data.apply(lambda r: 1 if pd.isnull(r['visitors']) else 0, axis=1)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    return all_data\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = get_grid(k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  create data frames with lag information\n",
    "\n",
    "Given gap, create training set with lag_gap, lagp_(gap+1) ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_lag(grid, lag_begin, lag_length ):\n",
    "    ''' \n",
    "    Add lag information to  grid to create training set\n",
    "    Specifically, given a row with day_ind = D, and lag_begin = 7, lag_end = 14\n",
    "    we add lag_7, lag_8, ..., lag_14 to this row   \n",
    "    \n",
    "    This is used to traing a model to forecast the visitors lag_begin days in the future\n",
    "    '''\n",
    "    index_cols = ['air_store_id', 'day_ind']\n",
    "    cols_to_rename = ['visitors', 'day_of_week', 'holiday_flg', 'holiday_eve', 'closed']\n",
    "    \n",
    "    grid_cp = grid.copy()\n",
    "    lag_end = lag_begin + lag_length - 1\n",
    "    for day_shift in range(lag_begin, lag_end + 1):\n",
    "        print('train day:', day_shift)\n",
    "        grid_shift = grid[index_cols + cols_to_rename].copy()\n",
    "        grid_shift['day_ind'] = grid_shift['day_ind'] + day_shift   \n",
    "        foo = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=foo)\n",
    "        grid = pd.merge(grid, grid_shift, on=index_cols, how='left')\n",
    "        del grid_shift\n",
    "    grid_train = grid[~pd.isnull(grid['visitors_lag_' + str(lag_end)])].copy()\n",
    "    grid_train = grid_train[grid_train['closed'] != 1]\n",
    "    grid_train.drop(['day_ind', 'month_ind', 'closed'], axis=1, inplace=True)\n",
    "\n",
    "    max_day_ind = np.max(grid.day_ind)\n",
    "    grid_test = grid_cp[grid_cp.day_ind == max_day_ind]\n",
    "    \n",
    "    f = lambda x: '{}_lag_{}'.format(x, str(lag_begin)) if x in cols_to_rename else x\n",
    "    grid_test = grid_test.rename(columns=f)\n",
    "  \n",
    "    for day_shift in range(lag_begin + 1, lag_end + 1):\n",
    "        print('test day:', day_shift)\n",
    "        grid_shift = grid_cp[grid_cp.day_ind == (max_day_ind - day_shift + lag_begin)][['air_store_id'] + cols_to_rename].copy()\n",
    "        f = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=f)\n",
    "        grid_test = pd.merge(grid_test, grid_shift, on='air_store_id')\n",
    "        del grid_shift       \n",
    "    grid_test.drop(['day_ind', 'month_ind'], axis=1, inplace=True)\n",
    "    return grid_train, grid_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatLabler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_cols):\n",
    "        self.cat_cols = cat_cols\n",
    "    def fit(self, X, y=None):\n",
    "        encoders = {}\n",
    "        for col in self.cat_cols:\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(X[col])\n",
    "            encoders[col] = encoder\n",
    "        self.encoders = encoders\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = X.copy()\n",
    "        for col in self.cat_cols:\n",
    "            X_new[col] = self.encoders[col].transform(X[col])\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all together to create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(k_grid=15, lag_length = 21, lag_begin0=1, lag_begin1=39):\n",
    "    '''\n",
    "    input:\n",
    "    k_grid -- create k weeks grid starting from the last date in air_visit_hist\n",
    "    k_train -- when creating the training data, if using the last_date in air_visit_hist as y, then X goes back to \n",
    "        last date - k_train * 7 day\n",
    "        Example: if k_grid = 10 and lag_length = 21, the last day_ind in air_visit_hist is 477,\n",
    "            then the grid will have every store with day_ind from 477 - 7*15 + 1 = 373 to 477.\n",
    "        Now since lag_length = 21, then the training data with LAG = 2 will have the following:\n",
    "        (i) day_ind = 477 as y, day_ind = 475, 474, ...., 475 - 21 + 1 = 475, 474, ..., 455 as X\n",
    "        (ii) day_ind = 476 as y, day_ind = 474, 473, ....., 454 as X\n",
    "        (iii) day_ind = 475 as y, day_ind = 473, 472, ...., 453 as X\n",
    "         ....\n",
    "         (x) day_ind = 464  as y, day_ind =  462                  408 as X\n",
    "     \n",
    "        The corresponding test/predict X_test and y_testwill be\n",
    "        y_test for  day_ind = 477 + LAG = 479\n",
    "        X_test consists of day_ind = 477, 476, ... 457\n",
    "     \n",
    "    lag: for each LAG between lag_begin0 and lag_begin1, create train set\n",
    "    X_train_lag and y_train_lag where X_train has lag from LAG to 7 * k - 1,\n",
    "    also create a test set X_test_lag \n",
    "    '''\n",
    "    grid = get_grid(k_grid)\n",
    "    last_train_day = np.max(air_visit_hist.day_ind)\n",
    "    \n",
    "    Data = {}\n",
    "    for lag_begin in range(lag_begin0, lag_begin1 + 1):\n",
    "        print('lag_begin=', lag_begin)\n",
    "        lag_end = lag_begin + lag_length - 1\n",
    "        gtrain, gtest = append_lag(grid, lag_begin, lag_length)\n",
    "        cat_columns = ['air_store_id', 'air_genre_name', 'air_area_name']  + ['day_of_week_lag_' +  str(lag) \n",
    "                                                                        for lag in range(lag_begin, lag_end + 1)]\n",
    "        catLabler = CatLabler(cat_columns)\n",
    "       \n",
    "        y_train = gtrain.visitors\n",
    "        X_train0 = gtrain[gtest.columns]\n",
    "        X_test0 = gtest\n",
    "       \n",
    "        catLabler.fit(X_train0)\n",
    "       \n",
    "        X_train1 = catLabler.transform(X_train0)\n",
    "      \n",
    "        X_test1 = catLabler.transform(X_test0)\n",
    "      \n",
    "        Data[lag_begin] = (y_train, X_train1, X_test1, X_test0.air_store_id)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.1  grid = 15, lag_length = 21, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data = create_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Data, open(data_dir + '/SubData15_21.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "rf = RandomForestRegressor(n_estimators=300)\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predict = rf.predict(X_test)\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_rf_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_rf = Result_rf_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_rf.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission2_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.2, grid = 15, lag_length = 21, ExtraTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "et = ExtraTreesRegressor(n_estimators=300)\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    et.fit(X_train, y_train)\n",
    "    y_predict = et.predict(X_test)\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_rf_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_rf = Result_rf_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_rf.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission2_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.3 Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pickle.load(open(data_dir + '/SubData15_21.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0]\ttrain-rmse:2.10166\n",
      "[50]\ttrain-rmse:0.498601\n",
      "[100]\ttrain-rmse:0.484249\n",
      "[150]\ttrain-rmse:0.472824\n",
      "2\n",
      "[0]\ttrain-rmse:2.10117\n",
      "[50]\ttrain-rmse:0.500068\n",
      "[100]\ttrain-rmse:0.484594\n",
      "[150]\ttrain-rmse:0.473195\n",
      "3\n",
      "[0]\ttrain-rmse:2.10354\n",
      "[50]\ttrain-rmse:0.49982\n",
      "[100]\ttrain-rmse:0.485468\n",
      "[150]\ttrain-rmse:0.473023\n",
      "[200]\ttrain-rmse:0.462507\n",
      "4\n",
      "[0]\ttrain-rmse:2.10573\n",
      "[50]\ttrain-rmse:0.499252\n",
      "[100]\ttrain-rmse:0.484961\n",
      "[150]\ttrain-rmse:0.473032\n",
      "5\n",
      "[0]\ttrain-rmse:2.10725\n",
      "[50]\ttrain-rmse:0.499533\n",
      "[100]\ttrain-rmse:0.484865\n",
      "[150]\ttrain-rmse:0.472543\n",
      "[200]\ttrain-rmse:0.461715\n",
      "6\n",
      "[0]\ttrain-rmse:2.10987\n",
      "[50]\ttrain-rmse:0.499754\n",
      "[100]\ttrain-rmse:0.484306\n",
      "[150]\ttrain-rmse:0.472144\n",
      "7\n",
      "[0]\ttrain-rmse:2.1105\n",
      "[50]\ttrain-rmse:0.501647\n",
      "[100]\ttrain-rmse:0.486622\n",
      "[150]\ttrain-rmse:0.474381\n",
      "[200]\ttrain-rmse:0.463868\n",
      "8\n",
      "[0]\ttrain-rmse:2.10816\n",
      "[50]\ttrain-rmse:0.505435\n",
      "[100]\ttrain-rmse:0.489964\n",
      "[150]\ttrain-rmse:0.477365\n",
      "9\n",
      "[0]\ttrain-rmse:2.1083\n",
      "[50]\ttrain-rmse:0.506427\n",
      "[100]\ttrain-rmse:0.491326\n",
      "[150]\ttrain-rmse:0.477573\n",
      "[200]\ttrain-rmse:0.46625\n",
      "10\n",
      "[0]\ttrain-rmse:2.11077\n",
      "[50]\ttrain-rmse:0.506684\n",
      "[100]\ttrain-rmse:0.491101\n",
      "[150]\ttrain-rmse:0.477614\n",
      "[200]\ttrain-rmse:0.465631\n",
      "11\n",
      "[0]\ttrain-rmse:2.1137\n",
      "[50]\ttrain-rmse:0.506316\n",
      "[100]\ttrain-rmse:0.491162\n",
      "[150]\ttrain-rmse:0.478399\n",
      "[200]\ttrain-rmse:0.466426\n",
      "12\n",
      "[0]\ttrain-rmse:2.1161\n",
      "[50]\ttrain-rmse:0.505586\n",
      "[100]\ttrain-rmse:0.490069\n",
      "[150]\ttrain-rmse:0.477042\n",
      "[200]\ttrain-rmse:0.465393\n",
      "13\n",
      "[0]\ttrain-rmse:2.11778\n",
      "[50]\ttrain-rmse:0.505152\n",
      "[100]\ttrain-rmse:0.4889\n",
      "[150]\ttrain-rmse:0.475932\n",
      "14\n",
      "[0]\ttrain-rmse:2.11803\n",
      "[50]\ttrain-rmse:0.507802\n",
      "[100]\ttrain-rmse:0.49022\n",
      "[150]\ttrain-rmse:0.476237\n",
      "[200]\ttrain-rmse:0.464293\n",
      "15\n",
      "[0]\ttrain-rmse:2.115\n",
      "[50]\ttrain-rmse:0.511387\n",
      "[100]\ttrain-rmse:0.492763\n",
      "[150]\ttrain-rmse:0.478402\n",
      "16\n",
      "[0]\ttrain-rmse:2.11427\n",
      "[50]\ttrain-rmse:0.509741\n",
      "[100]\ttrain-rmse:0.493862\n",
      "[150]\ttrain-rmse:0.479369\n",
      "[200]\ttrain-rmse:0.467417\n",
      "17\n",
      "[0]\ttrain-rmse:2.11755\n",
      "[50]\ttrain-rmse:0.508104\n",
      "[100]\ttrain-rmse:0.491886\n",
      "[150]\ttrain-rmse:0.477995\n",
      "[200]\ttrain-rmse:0.46418\n",
      "18\n",
      "[0]\ttrain-rmse:2.12133\n",
      "[50]\ttrain-rmse:0.508626\n",
      "[100]\ttrain-rmse:0.491413\n",
      "[150]\ttrain-rmse:0.475931\n",
      "[200]\ttrain-rmse:0.463815\n",
      "19\n",
      "[0]\ttrain-rmse:2.1206\n",
      "[50]\ttrain-rmse:0.508535\n",
      "[100]\ttrain-rmse:0.490308\n",
      "[150]\ttrain-rmse:0.477098\n",
      "[200]\ttrain-rmse:0.464722\n",
      "20\n",
      "[0]\ttrain-rmse:2.12282\n",
      "[50]\ttrain-rmse:0.508914\n",
      "[100]\ttrain-rmse:0.490413\n",
      "[150]\ttrain-rmse:0.475729\n",
      "21\n",
      "[0]\ttrain-rmse:2.12143\n",
      "[50]\ttrain-rmse:0.509287\n",
      "[100]\ttrain-rmse:0.491326\n",
      "[150]\ttrain-rmse:0.475699\n",
      "[200]\ttrain-rmse:0.461808\n",
      "22\n",
      "[0]\ttrain-rmse:2.11814\n",
      "[50]\ttrain-rmse:0.510851\n",
      "[100]\ttrain-rmse:0.49214\n",
      "[150]\ttrain-rmse:0.476979\n",
      "[200]\ttrain-rmse:0.463023\n",
      "[250]\ttrain-rmse:0.450706\n",
      "23\n",
      "[0]\ttrain-rmse:2.11801\n",
      "[50]\ttrain-rmse:0.511447\n",
      "[100]\ttrain-rmse:0.493162\n",
      "[150]\ttrain-rmse:0.477552\n",
      "[200]\ttrain-rmse:0.463968\n",
      "24\n",
      "[0]\ttrain-rmse:2.12064\n",
      "[50]\ttrain-rmse:0.510978\n",
      "[100]\ttrain-rmse:0.492686\n",
      "[150]\ttrain-rmse:0.475592\n",
      "[200]\ttrain-rmse:0.461621\n",
      "25\n",
      "[0]\ttrain-rmse:2.12291\n",
      "[50]\ttrain-rmse:0.510933\n",
      "[100]\ttrain-rmse:0.491639\n",
      "[150]\ttrain-rmse:0.476025\n",
      "[200]\ttrain-rmse:0.461762\n",
      "26\n",
      "[0]\ttrain-rmse:2.12414\n",
      "[50]\ttrain-rmse:0.510879\n",
      "[100]\ttrain-rmse:0.491995\n",
      "[150]\ttrain-rmse:0.476327\n",
      "[200]\ttrain-rmse:0.461413\n",
      "27\n",
      "[0]\ttrain-rmse:2.12629\n",
      "[50]\ttrain-rmse:0.511619\n",
      "[100]\ttrain-rmse:0.492714\n",
      "[150]\ttrain-rmse:0.475948\n",
      "[200]\ttrain-rmse:0.461128\n",
      "28\n",
      "[0]\ttrain-rmse:2.12466\n",
      "[50]\ttrain-rmse:0.513249\n",
      "[100]\ttrain-rmse:0.493927\n",
      "[150]\ttrain-rmse:0.477553\n",
      "[200]\ttrain-rmse:0.462918\n",
      "29\n",
      "[0]\ttrain-rmse:2.12089\n",
      "[50]\ttrain-rmse:0.516091\n",
      "[100]\ttrain-rmse:0.494529\n",
      "[150]\ttrain-rmse:0.477625\n",
      "[200]\ttrain-rmse:0.462237\n",
      "30\n",
      "[0]\ttrain-rmse:2.11938\n",
      "[50]\ttrain-rmse:0.516778\n",
      "[100]\ttrain-rmse:0.496009\n",
      "[150]\ttrain-rmse:0.477585\n",
      "[200]\ttrain-rmse:0.462794\n",
      "[250]\ttrain-rmse:0.448674\n",
      "31\n",
      "[0]\ttrain-rmse:2.12249\n",
      "[50]\ttrain-rmse:0.514041\n",
      "[100]\ttrain-rmse:0.492807\n",
      "[150]\ttrain-rmse:0.475315\n",
      "[200]\ttrain-rmse:0.459612\n",
      "[250]\ttrain-rmse:0.444281\n",
      "32\n",
      "[0]\ttrain-rmse:2.12523\n",
      "[50]\ttrain-rmse:0.514639\n",
      "[100]\ttrain-rmse:0.492781\n",
      "[150]\ttrain-rmse:0.474708\n",
      "[200]\ttrain-rmse:0.458858\n",
      "[250]\ttrain-rmse:0.444156\n",
      "33\n",
      "[0]\ttrain-rmse:2.127\n",
      "[50]\ttrain-rmse:0.514271\n",
      "[100]\ttrain-rmse:0.491502\n",
      "[150]\ttrain-rmse:0.472433\n",
      "[200]\ttrain-rmse:0.456768\n",
      "34\n",
      "[0]\ttrain-rmse:2.13048\n",
      "[50]\ttrain-rmse:0.51354\n",
      "[100]\ttrain-rmse:0.491724\n",
      "[150]\ttrain-rmse:0.472282\n",
      "[200]\ttrain-rmse:0.455137\n",
      "35\n",
      "[0]\ttrain-rmse:2.13115\n",
      "[50]\ttrain-rmse:0.512769\n",
      "[100]\ttrain-rmse:0.490825\n",
      "[150]\ttrain-rmse:0.471691\n",
      "[200]\ttrain-rmse:0.455878\n",
      "[250]\ttrain-rmse:0.439968\n",
      "36\n",
      "[0]\ttrain-rmse:2.12742\n",
      "[50]\ttrain-rmse:0.516932\n",
      "[100]\ttrain-rmse:0.493717\n",
      "[150]\ttrain-rmse:0.472464\n",
      "[200]\ttrain-rmse:0.455834\n",
      "[250]\ttrain-rmse:0.440978\n",
      "37\n",
      "[0]\ttrain-rmse:2.12717\n",
      "[50]\ttrain-rmse:0.515878\n",
      "[100]\ttrain-rmse:0.492591\n",
      "[150]\ttrain-rmse:0.473158\n",
      "[200]\ttrain-rmse:0.456143\n",
      "38\n",
      "[0]\ttrain-rmse:2.12962\n",
      "[50]\ttrain-rmse:0.516991\n",
      "[100]\ttrain-rmse:0.493463\n",
      "[150]\ttrain-rmse:0.474766\n",
      "[200]\ttrain-rmse:0.455427\n",
      "[250]\ttrain-rmse:0.439323\n",
      "39\n",
      "[0]\ttrain-rmse:2.1344\n",
      "[50]\ttrain-rmse:0.516513\n",
      "[100]\ttrain-rmse:0.492805\n",
      "[150]\ttrain-rmse:0.471731\n",
      "[200]\ttrain-rmse:0.452521\n",
      "[250]\ttrain-rmse:0.43673\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "xgb_params = {\n",
    "    'eta': 0.15,\n",
    "    'max_depth': 5,\n",
    "     'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    ly_train = np.log(y_train + 1)\n",
    "    xgtrain = xgb.DMatrix(X_train.values, ly_train)\n",
    "    xgtest = xgb.DMatrix(X_test.values)\n",
    "    cvresult = xgb.cv(xgb_params, xgtrain, num_boost_round=1000, nfold=5, metrics='rmse', early_stopping_rounds=50, verbose_eval=False)\n",
    "    num_rounds = cvresult.shape[0] + 55\n",
    "    model = xgb.train(xgb_params, xgtrain, num_boost_round=num_rounds, evals = [(xgtrain, 'train')], verbose_eval=50)\n",
    "    # make prediction\n",
    "    ly_predict = model.predict(xgtest)\n",
    "    y_predict = np.exp(ly_predict) - 1\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_df = Result_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_df.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission2_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.4 lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9\n",
    "    }\n",
    "\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    ly_train = np.log(y_train + 1)\n",
    "    lgb_train = lgb.Dataset(X_train, ly_train)\n",
    "    cv_lgb = lgb.cv(params, lgb_train, num_boost_round = 1000, nfold=5, stratified=False, early_stopping_rounds = 50, \n",
    "                verbose_eval = False)\n",
    "    num_rounds = len(cv_lgb['rmse-mean']) + 50\n",
    "    gbm = lgb.train(params,lgb_train,num_boost_round=num_rounds)\n",
    "    ly_predict = gbm.predict(X_test)\n",
    "    y_predict = np.exp(ly_predict) - 1\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_df = Result_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_df.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission2_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.5  Average of 2.3 and 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv(data_dir + '/submission2_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = pd.read_csv(data_dir + '/submission2_2.csv')\n",
    "sub3 = pd.read_csv(data_dir + '/submission2_3.csv')\n",
    "sub4 = pd.read_csv(data_dir + '/submission2_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34 = pd.merge(sub3, sub4, on='id', suffixes=['_3', '_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34['visitors'] = sub34.apply(lambda r: (r['visitors_3'] + r['visitors_4']) / 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34.drop(['visitors_3', 'visitors_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34.to_csv(data_dir + '/submission2_34.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.6 Average 2.2, 2.3 and 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34 = pd.merge(sub3, sub4, on='id', suffixes=['_3', '_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234 = pd.merge(sub2, sub34, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234['visitors'] = (sub234['visitors'] + sub234['visitors_3'] + sub234['visitors_4']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234.drop(['visitors_3', 'visitors_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234.to_csv(data_dir + '/submission2_234.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
