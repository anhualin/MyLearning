{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea:\n",
    "Given $k$, build a model to predict the number of visitors after k days using the following features:\n",
    "1. (holidayflag, day_of_week, is_closed, #visitors) for the past n weeks.\n",
    "2. store_id, gentre, area\n",
    "\n",
    "\n",
    "First do label encoding to categorical variables\n",
    "Then try one-hot encoding with the target weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Linux':\n",
    "    data_dir = '/home/alin/Data/Recruit_Holding'\n",
    "else:\n",
    "    data_dir = 'C:/Users/alin/Documents/Data/Recruit_Holding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load previous dump\n"
     ]
    }
   ],
   "source": [
    "DFS_dump = data_dir + '/DFS.p'\n",
    "if Path(DFS_dump).is_file():\n",
    "    print('load previous dump')\n",
    "    DFS = pickle.load(open(DFS_dump, 'rb'))\n",
    "    air_reserve = DFS['air_reserve']\n",
    "    air_reserve_day = DFS['air_reserve_day']\n",
    "    hpg_reserve = DFS['hpg_reserve']\n",
    "    hpg_reserve_day = DFS['hpg_reserve_day']\n",
    "    air_visit_hist = DFS['air_visit_hist']\n",
    "    date_info = DFS['date_info']\n",
    "    test = DFS['test']\n",
    "    air_store_info = DFS['air_store_info']\n",
    "    hpg_store_info = DFS['hpg_store_info']\n",
    "    store_id_relation = DFS['store_id_relation']\n",
    "    test = DFS['test']\n",
    "else:\n",
    "    print('run EDA1 first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the training and testing datasets before label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: add dates when a store is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(k = 3):\n",
    "    '''\n",
    "    Keep the last k weeks of air_vist_hist, then for any store missing on any day,  create the corresponding \n",
    "    row with expacted valud 0\n",
    "    '''\n",
    "    last_train_day = max(air_visit_hist.day_ind)\n",
    "    first_train_day = last_train_day - k * 7 + 1\n",
    "    \n",
    "    #filter into desire time frame\n",
    "    hist1 = air_visit_hist[(air_visit_hist.day_ind >= first_train_day) & (air_visit_hist.day_ind <= last_train_day)].copy()\n",
    "    all_stores = hist1.air_store_id.unique()\n",
    "    all_days = [i for i in range(first_train_day, last_train_day+1)]\n",
    "    \n",
    "    #create store x day grid\n",
    "    grid = np.array(list(product(*[all_stores, all_days])))\n",
    "    grid = pd.DataFrame(grid, columns=['air_store_id', 'day_ind_str' ])\n",
    "    grid['day_ind'] = grid.apply(lambda r: int(r['day_ind_str']), axis=1)\n",
    "    grid.drop('day_ind_str', axis=1, inplace=True)\n",
    "    \n",
    "    # add visit information \n",
    "    all_data = grid.merge(hist1, how='left', on=['air_store_id', 'day_ind'])\n",
    "    \n",
    "    # add date type information\n",
    "    all_data = all_data.merge(date_info, on='day_ind', suffixes=['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # add store information\n",
    "    all_data = all_data.merge(air_store_info, on = 'air_store_id', suffixes = ['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l'] + ['calendar_date', 'date', 'latitude', 'longitude', 'hpg_store_id']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # for those dates on which the visit informaiton of a store is missing, assume that it was closed abd with visit number 0\n",
    "    all_data['closed'] = all_data.apply(lambda r: 1 if pd.isnull(r['visitors']) else 0, axis=1)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    return all_data\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = get_grid(k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  create data frames with lag information\n",
    "\n",
    "Given gap, create training set with lag_gap, lagp_(gap+1) ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_lag(grid, lag_begin, lag_length ):\n",
    "    ''' \n",
    "    Add lag information to  grid to create training set\n",
    "    Specifically, given a row with day_ind = D, and lag_begin = 7, lag_end = 14\n",
    "    we add lag_7, lag_8, ..., lag_14 to this row   \n",
    "    \n",
    "    This is used to traing a model to forecast the visitors lag_begin days in the future\n",
    "    '''\n",
    "    index_cols = ['air_store_id', 'day_ind']\n",
    "    cols_to_rename = ['visitors', 'day_of_week', 'holiday_flg', 'holiday_eve', 'closed']\n",
    "    \n",
    "    grid_cp = grid.copy()\n",
    "    lag_end = lag_begin + lag_length - 1\n",
    "    for day_shift in range(lag_begin, lag_end + 1):\n",
    "        print('train day:', day_shift)\n",
    "        grid_shift = grid[index_cols + cols_to_rename].copy()\n",
    "        grid_shift['day_ind'] = grid_shift['day_ind'] + day_shift   \n",
    "        foo = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=foo)\n",
    "        grid = pd.merge(grid, grid_shift, on=index_cols, how='left')\n",
    "        del grid_shift\n",
    "    grid_train = grid[~pd.isnull(grid['visitors_lag_' + str(lag_end)])].copy()\n",
    "    grid_train = grid_train[grid_train['closed'] != 1]\n",
    "    grid_train.drop(['day_ind', 'month_ind', 'closed'], axis=1, inplace=True)\n",
    "\n",
    "    max_day_ind = np.max(grid.day_ind)\n",
    "    grid_test = grid_cp[grid_cp.day_ind == max_day_ind]\n",
    "    \n",
    "    f = lambda x: '{}_lag_{}'.format(x, str(lag_begin)) if x in cols_to_rename else x\n",
    "    grid_test = grid_test.rename(columns=f)\n",
    "    grid_test['target_day_ind'] = grid_test['day_ind'] + lag_begin   \n",
    "    for day_shift in range(lag_begin + 1, lag_end + 1):\n",
    "        print('test day:', day_shift)\n",
    "        grid_shift = grid_cp[grid_cp.day_ind == (max_day_ind - day_shift + lag_begin)][['air_store_id'] + cols_to_rename].copy()\n",
    "        f = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=f)\n",
    "        grid_test = pd.merge(grid_test, grid_shift, on='air_store_id')\n",
    "        del grid_shift\n",
    "    grid_test.drop(['day_ind', 'month_ind'], axis=1, inplace=True)\n",
    "    grid_test = pd.merge(grid_test, date_info, left_on='target_day_ind', right_on = 'day_ind')\n",
    "    grid_test.drop(['target_day_ind', 'calendar_date', 'date', 'day_ind', 'month_ind'], axis=1, inplace=True)   \n",
    "    return grid_train, grid_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatLabler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_cols, lag_begin, lag_length):\n",
    "        self.cat_cols = cat_cols\n",
    "        self.lag_begin = lag_begin\n",
    "        self.lag_length = lag_length\n",
    "    def fit(self, X, y=None):\n",
    "        encoders = {}\n",
    "        self.weekday_encoder = LabelEncoder()\n",
    "        self.weekday_encoder.fit(['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'])\n",
    "        for col in self.cat_cols:\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(X[col])\n",
    "            encoders[col] = encoder\n",
    "        self.encoders = encoders\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = X.copy()\n",
    "        for col in self.cat_cols:\n",
    "            X_new[col] = self.encoders[col].transform(X[col])\n",
    "        X_new['day_of_week'] = self.weekday_encoder.transform(X['day_of_week'])\n",
    "        for i in range(self.lag_begin, self.lag_begin + self.lag_length):\n",
    "            X_new['day_of_week_lag_'+str(i)] = self.weekday_encoder.transform(X['day_of_week_lag_'+str(i)])\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all together to create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(k_grid=15, lag_length = 21, lag_begin0=1, lag_begin1=39):\n",
    "    '''\n",
    "    input:\n",
    "    k_grid -- create k weeks grid starting from the last date in air_visit_hist\n",
    "    k_train -- when creating the training data, if using the last_date in air_visit_hist as y, then X goes back to \n",
    "        last date - k_train * 7 day\n",
    "        Example: if k_grid = 10 and lag_length = 21, the last day_ind in air_visit_hist is 477,\n",
    "            then the grid will have every store with day_ind from 477 - 7*15 + 1 = 373 to 477.\n",
    "        Now since lag_length = 21, then the training data with LAG = 2 will have the following:\n",
    "        (i) day_ind = 477 as y, day_ind = 475, 474, ...., 475 - 21 + 1 = 475, 474, ..., 455 as X\n",
    "        (ii) day_ind = 476 as y, day_ind = 474, 473, ....., 454 as X\n",
    "        (iii) day_ind = 475 as y, day_ind = 473, 472, ...., 453 as X\n",
    "         ....\n",
    "         (x) day_ind = 464  as y, day_ind =  462                  408 as X\n",
    "     \n",
    "        The corresponding test/predict X_test and y_testwill be\n",
    "        y_test for  day_ind = 477 + LAG = 479\n",
    "        X_test consists of day_ind = 477, 476, ... 457\n",
    "     \n",
    "    lag: for each LAG between lag_begin0 and lag_begin1, create train set\n",
    "    X_train_lag and y_train_lag where X_train has lag from LAG to 7 * k - 1,\n",
    "    also create a test set X_test_lag \n",
    "    '''\n",
    "    grid = get_grid(k_grid)\n",
    "    last_train_day = np.max(air_visit_hist.day_ind)\n",
    "    \n",
    "    Data = {}\n",
    "    for lag_begin in range(lag_begin0, lag_begin1 + 1):\n",
    "        print('lag_begin=', lag_begin)\n",
    "        lag_end = lag_begin + lag_length - 1\n",
    "        gtrain, gtest = append_lag(grid, lag_begin, lag_length)\n",
    "        cat_columns = ['air_store_id', 'air_genre_name', 'air_area_name']  \n",
    "        catLabler = CatLabler(cat_columns, lag_begin, lag_length)\n",
    "       \n",
    "        y_train = gtrain.visitors\n",
    "        X_train0 = gtrain[gtest.columns]\n",
    "        X_test0 = gtest\n",
    "       \n",
    "        catLabler.fit(X_train0)\n",
    "       \n",
    "        X_train1 = catLabler.transform(X_train0)\n",
    "      \n",
    "        X_test1 = catLabler.transform(X_test0)\n",
    "      \n",
    "        Data[lag_begin] = (y_train, X_train1, X_test1, X_test0.air_store_id)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data = create_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 3.1 Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data = pickle.load(open(data_dir + '/SubData15_21.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alin/.pyenv/versions/3.6.3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0]\ttrain-rmse:2.10166\n",
      "[50]\ttrain-rmse:0.49801\n",
      "[100]\ttrain-rmse:0.483414\n",
      "[150]\ttrain-rmse:0.471867\n",
      "[200]\ttrain-rmse:0.462073\n",
      "2\n",
      "[0]\ttrain-rmse:2.10117\n",
      "[50]\ttrain-rmse:0.499102\n",
      "[100]\ttrain-rmse:0.484733\n",
      "[150]\ttrain-rmse:0.472302\n",
      "3\n",
      "[0]\ttrain-rmse:2.10354\n",
      "[50]\ttrain-rmse:0.498371\n",
      "[100]\ttrain-rmse:0.483827\n",
      "[150]\ttrain-rmse:0.472005\n",
      "4\n",
      "[0]\ttrain-rmse:2.10573\n",
      "[50]\ttrain-rmse:0.497381\n",
      "[100]\ttrain-rmse:0.483144\n",
      "[150]\ttrain-rmse:0.47144\n",
      "5\n",
      "[0]\ttrain-rmse:2.10725\n",
      "[50]\ttrain-rmse:0.498383\n",
      "[100]\ttrain-rmse:0.483273\n",
      "[150]\ttrain-rmse:0.471738\n",
      "6\n",
      "[0]\ttrain-rmse:2.10987\n",
      "[50]\ttrain-rmse:0.499048\n",
      "[100]\ttrain-rmse:0.484117\n",
      "[150]\ttrain-rmse:0.472047\n",
      "[200]\ttrain-rmse:0.461562\n",
      "7\n",
      "[0]\ttrain-rmse:2.11003\n",
      "[50]\ttrain-rmse:0.501465\n",
      "[100]\ttrain-rmse:0.4861\n",
      "[150]\ttrain-rmse:0.473636\n",
      "8\n",
      "[0]\ttrain-rmse:2.10816\n",
      "[50]\ttrain-rmse:0.50421\n",
      "[100]\ttrain-rmse:0.488085\n",
      "[150]\ttrain-rmse:0.475377\n",
      "9\n",
      "[0]\ttrain-rmse:2.1083\n",
      "[50]\ttrain-rmse:0.505586\n",
      "[100]\ttrain-rmse:0.489872\n",
      "[150]\ttrain-rmse:0.477791\n",
      "10\n",
      "[0]\ttrain-rmse:2.11077\n",
      "[50]\ttrain-rmse:0.505097\n",
      "[100]\ttrain-rmse:0.488835\n",
      "[150]\ttrain-rmse:0.475874\n",
      "11\n",
      "[0]\ttrain-rmse:2.1137\n",
      "[50]\ttrain-rmse:0.506048\n",
      "[100]\ttrain-rmse:0.489743\n",
      "[150]\ttrain-rmse:0.47657\n",
      "12\n",
      "[0]\ttrain-rmse:2.11624\n",
      "[50]\ttrain-rmse:0.505223\n",
      "[100]\ttrain-rmse:0.489848\n",
      "[150]\ttrain-rmse:0.477017\n",
      "13\n",
      "[0]\ttrain-rmse:2.11779\n",
      "[50]\ttrain-rmse:0.505148\n",
      "[100]\ttrain-rmse:0.488229\n",
      "[150]\ttrain-rmse:0.475757\n",
      "14\n",
      "[0]\ttrain-rmse:2.11808\n",
      "[50]\ttrain-rmse:0.506266\n",
      "[100]\ttrain-rmse:0.49018\n",
      "[150]\ttrain-rmse:0.47663\n",
      "[200]\ttrain-rmse:0.46393\n",
      "15\n",
      "[0]\ttrain-rmse:2.115\n",
      "[50]\ttrain-rmse:0.509704\n",
      "[100]\ttrain-rmse:0.492832\n",
      "[150]\ttrain-rmse:0.47884\n",
      "16\n",
      "[0]\ttrain-rmse:2.11427\n",
      "[50]\ttrain-rmse:0.508631\n",
      "[100]\ttrain-rmse:0.491811\n",
      "[150]\ttrain-rmse:0.478296\n",
      "[200]\ttrain-rmse:0.465363\n",
      "17\n",
      "[0]\ttrain-rmse:2.11755\n",
      "[50]\ttrain-rmse:0.508191\n",
      "[100]\ttrain-rmse:0.491168\n",
      "[150]\ttrain-rmse:0.477064\n",
      "[200]\ttrain-rmse:0.464457\n",
      "18\n",
      "[0]\ttrain-rmse:2.12132\n",
      "[50]\ttrain-rmse:0.508904\n",
      "[100]\ttrain-rmse:0.491285\n",
      "[150]\ttrain-rmse:0.477135\n",
      "[200]\ttrain-rmse:0.463634\n",
      "19\n",
      "[0]\ttrain-rmse:2.1206\n",
      "[50]\ttrain-rmse:0.508403\n",
      "[100]\ttrain-rmse:0.491295\n",
      "[150]\ttrain-rmse:0.475945\n",
      "[200]\ttrain-rmse:0.462445\n",
      "20\n",
      "[0]\ttrain-rmse:2.12282\n",
      "[50]\ttrain-rmse:0.508341\n",
      "[100]\ttrain-rmse:0.490127\n",
      "[150]\ttrain-rmse:0.474614\n",
      "[200]\ttrain-rmse:0.461508\n",
      "21\n",
      "[0]\ttrain-rmse:2.12149\n",
      "[50]\ttrain-rmse:0.509601\n",
      "[100]\ttrain-rmse:0.491779\n",
      "[150]\ttrain-rmse:0.47735\n",
      "[200]\ttrain-rmse:0.46437\n",
      "22\n",
      "[0]\ttrain-rmse:2.11814\n",
      "[50]\ttrain-rmse:0.510823\n",
      "[100]\ttrain-rmse:0.492606\n",
      "[150]\ttrain-rmse:0.477421\n",
      "[200]\ttrain-rmse:0.462853\n",
      "23\n",
      "[0]\ttrain-rmse:2.11714\n",
      "[50]\ttrain-rmse:0.511727\n",
      "[100]\ttrain-rmse:0.493922\n",
      "[150]\ttrain-rmse:0.478722\n",
      "[200]\ttrain-rmse:0.464813\n",
      "24\n",
      "[0]\ttrain-rmse:2.12108\n",
      "[50]\ttrain-rmse:0.511279\n",
      "[100]\ttrain-rmse:0.49305\n",
      "[150]\ttrain-rmse:0.476714\n",
      "[200]\ttrain-rmse:0.462376\n",
      "[250]\ttrain-rmse:0.449156\n",
      "25\n",
      "[0]\ttrain-rmse:2.12291\n",
      "[50]\ttrain-rmse:0.510955\n",
      "[100]\ttrain-rmse:0.491934\n",
      "[150]\ttrain-rmse:0.475079\n",
      "[200]\ttrain-rmse:0.460653\n",
      "26\n",
      "[0]\ttrain-rmse:2.12414\n",
      "[50]\ttrain-rmse:0.512304\n",
      "[100]\ttrain-rmse:0.492664\n",
      "[150]\ttrain-rmse:0.476348\n",
      "[200]\ttrain-rmse:0.461189\n",
      "[250]\ttrain-rmse:0.448295\n",
      "27\n",
      "[0]\ttrain-rmse:2.1262\n",
      "[50]\ttrain-rmse:0.512633\n",
      "[100]\ttrain-rmse:0.492121\n",
      "[150]\ttrain-rmse:0.47546\n",
      "[200]\ttrain-rmse:0.460664\n",
      "28\n",
      "[0]\ttrain-rmse:2.12466\n",
      "[50]\ttrain-rmse:0.512525\n",
      "[100]\ttrain-rmse:0.492818\n",
      "[150]\ttrain-rmse:0.475986\n",
      "29\n",
      "[0]\ttrain-rmse:2.12085\n",
      "[50]\ttrain-rmse:0.514123\n",
      "[100]\ttrain-rmse:0.495055\n",
      "[150]\ttrain-rmse:0.478635\n",
      "[200]\ttrain-rmse:0.463687\n",
      "30\n",
      "[0]\ttrain-rmse:2.11936\n",
      "[50]\ttrain-rmse:0.515906\n",
      "[100]\ttrain-rmse:0.495162\n",
      "[150]\ttrain-rmse:0.477701\n",
      "[200]\ttrain-rmse:0.461786\n",
      "[250]\ttrain-rmse:0.446698\n",
      "31\n",
      "[0]\ttrain-rmse:2.12249\n",
      "[50]\ttrain-rmse:0.515367\n",
      "[100]\ttrain-rmse:0.493046\n",
      "[150]\ttrain-rmse:0.475875\n",
      "[200]\ttrain-rmse:0.459921\n",
      "32\n",
      "[0]\ttrain-rmse:2.12523\n",
      "[50]\ttrain-rmse:0.514349\n",
      "[100]\ttrain-rmse:0.492788\n",
      "[150]\ttrain-rmse:0.475604\n",
      "[200]\ttrain-rmse:0.459236\n",
      "33\n",
      "[0]\ttrain-rmse:2.127\n",
      "[50]\ttrain-rmse:0.514288\n",
      "[100]\ttrain-rmse:0.492777\n",
      "[150]\ttrain-rmse:0.474513\n",
      "[200]\ttrain-rmse:0.458404\n",
      "34\n",
      "[0]\ttrain-rmse:2.13086\n",
      "[50]\ttrain-rmse:0.513993\n",
      "[100]\ttrain-rmse:0.491572\n",
      "[150]\ttrain-rmse:0.473744\n",
      "[200]\ttrain-rmse:0.456513\n",
      "[250]\ttrain-rmse:0.441227\n",
      "35\n",
      "[0]\ttrain-rmse:2.13115\n",
      "[50]\ttrain-rmse:0.513511\n",
      "[100]\ttrain-rmse:0.491156\n",
      "[150]\ttrain-rmse:0.471824\n",
      "[200]\ttrain-rmse:0.456234\n",
      "36\n",
      "[0]\ttrain-rmse:2.12689\n",
      "[50]\ttrain-rmse:0.517651\n",
      "[100]\ttrain-rmse:0.494772\n",
      "[150]\ttrain-rmse:0.474984\n",
      "37\n",
      "[0]\ttrain-rmse:2.12717\n",
      "[50]\ttrain-rmse:0.515745\n",
      "[100]\ttrain-rmse:0.49249\n",
      "[150]\ttrain-rmse:0.472219\n",
      "[200]\ttrain-rmse:0.455411\n",
      "38\n",
      "[0]\ttrain-rmse:2.12962\n",
      "[50]\ttrain-rmse:0.516095\n",
      "[100]\ttrain-rmse:0.492846\n",
      "[150]\ttrain-rmse:0.473184\n",
      "[200]\ttrain-rmse:0.454924\n",
      "39\n",
      "[0]\ttrain-rmse:2.1344\n",
      "[50]\ttrain-rmse:0.516775\n",
      "[100]\ttrain-rmse:0.491632\n",
      "[150]\ttrain-rmse:0.471134\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "xgb_params = {\n",
    "    'eta': 0.15,\n",
    "    'max_depth': 5,\n",
    "     'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    ly_train = np.log(y_train + 1)\n",
    "    xgtrain = xgb.DMatrix(X_train.values, ly_train)\n",
    "    xgtest = xgb.DMatrix(X_test.values)\n",
    "    cvresult = xgb.cv(xgb_params, xgtrain, num_boost_round=1000, nfold=5, metrics='rmse', early_stopping_rounds=50, verbose_eval=False)\n",
    "    num_rounds = cvresult.shape[0] + 55\n",
    "    model = xgb.train(xgb_params, xgtrain, num_boost_round=num_rounds, evals = [(xgtrain, 'train')], verbose_eval=50)\n",
    "    # make prediction\n",
    "    ly_predict = model.predict(xgtest)\n",
    "    y_predict = np.exp(ly_predict) - 1\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_df = Result_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_df.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission3_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.4 lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9\n",
    "    }\n",
    "\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    ly_train = np.log(y_train + 1)\n",
    "    lgb_train = lgb.Dataset(X_train, ly_train)\n",
    "    cv_lgb = lgb.cv(params, lgb_train, num_boost_round = 1000, nfold=5, stratified=False, early_stopping_rounds = 50, \n",
    "                verbose_eval = False)\n",
    "    num_rounds = len(cv_lgb['rmse-mean']) + 50\n",
    "    gbm = lgb.train(params,lgb_train,num_boost_round=num_rounds)\n",
    "    ly_predict = gbm.predict(X_test)\n",
    "    y_predict = np.exp(ly_predict) - 1\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_df = Result_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_df.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission2_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.5  Average of 2.3 and 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv(data_dir + '/submission2_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = pd.read_csv(data_dir + '/submission2_2.csv')\n",
    "sub3 = pd.read_csv(data_dir + '/submission2_3.csv')\n",
    "sub4 = pd.read_csv(data_dir + '/submission2_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34 = pd.merge(sub3, sub4, on='id', suffixes=['_3', '_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34['visitors'] = sub34.apply(lambda r: (r['visitors_3'] + r['visitors_4']) / 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34.drop(['visitors_3', 'visitors_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34.to_csv(data_dir + '/submission2_34.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.6 Average 2.2, 2.3 and 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub34 = pd.merge(sub3, sub4, on='id', suffixes=['_3', '_4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234 = pd.merge(sub2, sub34, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234['visitors'] = (sub234['visitors'] + sub234['visitors_3'] + sub234['visitors_4']) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234.drop(['visitors_3', 'visitors_4'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub234.to_csv(data_dir + '/submission2_234.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
