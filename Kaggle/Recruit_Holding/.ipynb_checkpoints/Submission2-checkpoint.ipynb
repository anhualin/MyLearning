{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea:\n",
    "Given $k$, build a model to predict the number of visitors after k days using the following features:\n",
    "1. (holidayflag, day_of_week, is_closed, #visitors) for the past n weeks.\n",
    "2. store_id, gentre, area\n",
    "\n",
    "Only do label encoding to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Linux':\n",
    "    data_dir = '/home/alin/Data/Recruit_Holding'\n",
    "else:\n",
    "    data_dir = 'C:/Users/alin/Documents/Data/Recruit_Holding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load previous dump\n"
     ]
    }
   ],
   "source": [
    "DFS_dump = data_dir + '/DFS.p'\n",
    "if Path(DFS_dump).is_file():\n",
    "    print('load previous dump')\n",
    "    DFS = pickle.load(open(DFS_dump, 'rb'))\n",
    "    air_reserve = DFS['air_reserve']\n",
    "    air_reserve_day = DFS['air_reserve_day']\n",
    "    hpg_reserve = DFS['hpg_reserve']\n",
    "    hpg_reserve_day = DFS['hpg_reserve_day']\n",
    "    air_visit_hist = DFS['air_visit_hist']\n",
    "    date_info = DFS['date_info']\n",
    "    test = DFS['test']\n",
    "    air_store_info = DFS['air_store_info']\n",
    "    hpg_store_info = DFS['hpg_store_info']\n",
    "    store_id_relation = DFS['store_id_relation']\n",
    "    test = DFS['test']\n",
    "else:\n",
    "    print('run EDA1 first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the training and testing datasets before label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: add dates when a store is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(k = 3):\n",
    "    '''\n",
    "    Keep the last k weeks of air_vist_hist, then for any store missing on any day,  create the corresponding \n",
    "    row with expacted valud 0\n",
    "    '''\n",
    "    last_train_day = max(air_visit_hist.day_ind)\n",
    "    first_train_day = last_train_day - k * 7 + 1\n",
    "    \n",
    "    #filter into desire time frame\n",
    "    hist1 = air_visit_hist[(air_visit_hist.day_ind >= first_train_day) & (air_visit_hist.day_ind <= last_train_day)].copy()\n",
    "    all_stores = hist1.air_store_id.unique()\n",
    "    all_days = [i for i in range(first_train_day, last_train_day+1)]\n",
    "    \n",
    "    #create store x day grid\n",
    "    grid = np.array(list(product(*[all_stores, all_days])))\n",
    "    grid = pd.DataFrame(grid, columns=['air_store_id', 'day_ind_str' ])\n",
    "    grid['day_ind'] = grid.apply(lambda r: int(r['day_ind_str']), axis=1)\n",
    "    grid.drop('day_ind_str', axis=1, inplace=True)\n",
    "    \n",
    "    # add visit information \n",
    "    all_data = grid.merge(hist1, how='left', on=['air_store_id', 'day_ind'])\n",
    "    \n",
    "    # add date type information\n",
    "    all_data = all_data.merge(date_info, on='day_ind', suffixes=['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # add store information\n",
    "    all_data = all_data.merge(air_store_info, on = 'air_store_id', suffixes = ['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l'] + ['calendar_date', 'date', 'latitude', 'longitude', 'hpg_store_id']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # for those dates on which the visit informaiton of a store is missing, assume that it was closed abd with visit number 0\n",
    "    all_data['closed'] = all_data.apply(lambda r: 1 if pd.isnull(r['visitors']) else 0, axis=1)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    return all_data\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = get_grid(k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  create data frames with lag information\n",
    "\n",
    "Given gap, create training set with lag_gap, lagp_(gap+1) ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_lag(grid, lag_begin, lag_length ):\n",
    "    ''' \n",
    "    Add lag information to  grid to create training set\n",
    "    Specifically, given a row with day_ind = D, and lag_begin = 7, lag_end = 14\n",
    "    we add lag_7, lag_8, ..., lag_14 to this row   \n",
    "    \n",
    "    This is used to traing a model to forecast the visitors lag_begin days in the future\n",
    "    '''\n",
    "    index_cols = ['air_store_id', 'day_ind']\n",
    "    cols_to_rename = ['visitors', 'day_of_week', 'holiday_flg', 'holiday_eve', 'closed']\n",
    "    \n",
    "    grid_cp = grid.copy()\n",
    "    lag_end = lag_begin + lag_length - 1\n",
    "    for day_shift in range(lag_begin, lag_end + 1):\n",
    "        print('train day:', day_shift)\n",
    "        grid_shift = grid[index_cols + cols_to_rename].copy()\n",
    "        grid_shift['day_ind'] = grid_shift['day_ind'] + day_shift   \n",
    "        foo = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=foo)\n",
    "        grid = pd.merge(grid, grid_shift, on=index_cols, how='left')\n",
    "        del grid_shift\n",
    "    grid_train = grid[~pd.isnull(grid['visitors_lag_' + str(lag_end)])].copy()\n",
    "    grid_train = grid_train[grid_train['closed'] != 1]\n",
    "    grid_train.drop(['day_ind', 'month_ind', 'closed'], axis=1, inplace=True)\n",
    "\n",
    "    max_day_ind = np.max(grid.day_ind)\n",
    "    grid_test = grid_cp[grid_cp.day_ind == max_day_ind]\n",
    "    \n",
    "    f = lambda x: '{}_lag_{}'.format(x, str(lag_begin)) if x in cols_to_rename else x\n",
    "    grid_test = grid_test.rename(columns=f)\n",
    "  \n",
    "    for day_shift in range(lag_begin + 1, lag_end + 1):\n",
    "        print('test day:', day_shift)\n",
    "        grid_shift = grid_cp[grid_cp.day_ind == (max_day_ind - day_shift + lag_begin)][['air_store_id'] + cols_to_rename].copy()\n",
    "        f = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=f)\n",
    "        grid_test = pd.merge(grid_test, grid_shift, on='air_store_id')\n",
    "        del grid_shift       \n",
    "    grid_test.drop(['day_ind', 'month_ind'], axis=1, inplace=True)\n",
    "    return grid_train, grid_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatLabler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_cols):\n",
    "        self.cat_cols = cat_cols\n",
    "    def fit(self, X, y=None):\n",
    "        encoders = {}\n",
    "        for col in self.cat_cols:\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(X[col])\n",
    "            encoders[col] = encoder\n",
    "        self.encoders = encoders\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = X.copy()\n",
    "        for col in self.cat_cols:\n",
    "            X_new[col] = self.encoders[col].transform(X[col])\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all together to create final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(k_grid=15, lag_length = 21, lag_begin0=1, lag_begin1=39):\n",
    "    '''\n",
    "    input:\n",
    "    k_grid -- create k weeks grid starting from the last date in air_visit_hist\n",
    "    k_train -- when creating the training data, if using the last_date in air_visit_hist as y, then X goes back to \n",
    "        last date - k_train * 7 day\n",
    "        Example: if k_grid = 10 and lag_length = 21, the last day_ind in air_visit_hist is 477,\n",
    "            then the grid will have every store with day_ind from 477 - 7*15 + 1 = 373 to 477.\n",
    "        Now since lag_length = 21, then the training data with LAG = 2 will have the following:\n",
    "        (i) day_ind = 477 as y, day_ind = 475, 474, ...., 475 - 21 + 1 = 475, 474, ..., 455 as X\n",
    "        (ii) day_ind = 476 as y, day_ind = 474, 473, ....., 454 as X\n",
    "        (iii) day_ind = 475 as y, day_ind = 473, 472, ...., 453 as X\n",
    "         ....\n",
    "         (x) day_ind = 464  as y, day_ind =  462                  408 as X\n",
    "     \n",
    "        The corresponding test/predict X_test and y_testwill be\n",
    "        y_test for  day_ind = 477 + LAG = 479\n",
    "        X_test consists of day_ind = 477, 476, ... 457\n",
    "     \n",
    "    lag: for each LAG between lag_begin0 and lag_begin1, create train set\n",
    "    X_train_lag and y_train_lag where X_train has lag from LAG to 7 * k - 1,\n",
    "    also create a test set X_test_lag \n",
    "    '''\n",
    "    grid = get_grid(k_grid)\n",
    "    last_train_day = np.max(air_visit_hist.day_ind)\n",
    "    \n",
    "    Data = {}\n",
    "    for lag_begin in range(lag_begin0, lag_begin1 + 1):\n",
    "        print('lag_begin=', lag_begin)\n",
    "        lag_end = lag_begin + lag_length - 1\n",
    "        gtrain, gtest = append_lag(grid, lag_begin, lag_length)\n",
    "        cat_columns = ['air_store_id', 'air_genre_name', 'air_area_name']  + ['day_of_week_lag_' +  str(lag) \n",
    "                                                                        for lag in range(lag_begin, lag_end + 1)]\n",
    "        catLabler = CatLabler(cat_columns)\n",
    "       \n",
    "        y_train = gtrain.visitors\n",
    "        X_train0 = gtrain[gtest.columns]\n",
    "        X_test0 = gtest\n",
    "       \n",
    "        catLabler.fit(X_train0)\n",
    "       \n",
    "        X_train1 = catLabler.transform(X_train0)\n",
    "      \n",
    "        X_test1 = catLabler.transform(X_test0)\n",
    "      \n",
    "        Data[lag_begin] = (y_train, X_train1, X_test1, X_test0.air_store_id)\n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.1  grid = 15, lag_length = 21, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data = create_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Data, open(data_dir + '/SubData15_21.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "rf = RandomForestRegressor(n_estimators=300)\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_predict = rf.predict(X_test)\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_rf_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_rf = Result_rf_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_rf.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission2_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2.2, grid = 15, lag_length = 21, ExtraTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "last_train_date = np.max(air_visit_hist.day_ind)\n",
    "Results = []\n",
    "et = ExtraTreesRegressor(n_estimators=300)\n",
    "for i in range(1, 40):\n",
    "    print(i)\n",
    "    y_train, X_train, X_test, stores = Data[i]\n",
    "    et.fit(X_train, y_train)\n",
    "    y_predict = et.predict(X_test)\n",
    "    rdf = pd.DataFrame({'id': stores, 'visitors': y_predict})\n",
    "    dt = date_info[date_info.day_ind == (last_train_date + i)]\n",
    "    rdf['id'] = rdf.apply(lambda r: r['id'] + '_' + dt.calendar_date, axis=1)\n",
    "    Results.append(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_rf_df = pd.concat(Results)\n",
    "\n",
    "sample_submission = pd.read_csv(data_dir + '/sample_submission.csv.zip')\n",
    "sample_submission['ord'] = sample_submission.index\n",
    "\n",
    "output_rf = Result_rf_df.merge(sample_submission, on='id', suffixes=['','_r'])[['id', 'visitors', 'ord']]\n",
    "\n",
    "output = output_rf.sort_values('ord', axis=0)[['id', 'visitors']]\n",
    "output.to_csv(data_dir + '/submission2_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train, X_test, stores = Data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "ly_train = np.log(y_train+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6, 27):\n",
    "    var = 'visitors_lag_' + str(i)\n",
    "    X_train[var] = np.log(X_train[var] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0, X_test0, ly_train0, ly_test0 = train_test_split(X_train, ly_train, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0_v = X_train0.values\n",
    "X_test0_v = X_test0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(X_train0_v, ly_train0)\n",
    "xgtest = xgb.DMatrix(X_test0_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'eta': 0.15,\n",
    "    'max_depth': 5,\n",
    "     'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.13025+0.00210966\ttest-rmse:2.13033+0.0105536\n",
      "[1]\ttrain-rmse:1.86154+0.00192198\ttest-rmse:1.86181+0.010592\n",
      "[2]\ttrain-rmse:1.63991+0.00185997\ttest-rmse:1.64044+0.0105317\n",
      "[3]\ttrain-rmse:1.45933+0.00154265\ttest-rmse:1.46034+0.0105838\n",
      "[4]\ttrain-rmse:1.31337+0.00149331\ttest-rmse:1.31486+0.0100994\n",
      "[5]\ttrain-rmse:1.19658+0.00129722\ttest-rmse:1.19868+0.00995538\n",
      "[6]\ttrain-rmse:1.10441+0.0014049\ttest-rmse:1.1072+0.0094529\n",
      "[7]\ttrain-rmse:1.0325+0.00154377\ttest-rmse:1.0361+0.00892129\n",
      "[8]\ttrain-rmse:0.977032+0.00161218\ttest-rmse:0.981428+0.00841172\n",
      "[9]\ttrain-rmse:0.934668+0.00154461\ttest-rmse:0.939851+0.00793404\n",
      "[10]\ttrain-rmse:0.902754+0.00160308\ttest-rmse:0.908679+0.00747872\n",
      "[11]\ttrain-rmse:0.878864+0.00149739\ttest-rmse:0.885551+0.00722585\n",
      "[12]\ttrain-rmse:0.860996+0.00146218\ttest-rmse:0.86842+0.00691109\n",
      "[13]\ttrain-rmse:0.847691+0.00134742\ttest-rmse:0.855863+0.00664895\n",
      "[14]\ttrain-rmse:0.837695+0.0012638\ttest-rmse:0.846738+0.00639854\n",
      "[15]\ttrain-rmse:0.830351+0.00126153\ttest-rmse:0.840077+0.00625369\n",
      "[16]\ttrain-rmse:0.824883+0.00117933\ttest-rmse:0.835291+0.00607489\n",
      "[17]\ttrain-rmse:0.820714+0.001339\ttest-rmse:0.831996+0.00601391\n",
      "[18]\ttrain-rmse:0.817487+0.00137803\ttest-rmse:0.829462+0.00597812\n",
      "[19]\ttrain-rmse:0.814977+0.00138792\ttest-rmse:0.827608+0.00595144\n",
      "[20]\ttrain-rmse:0.81317+0.00147483\ttest-rmse:0.82632+0.00590117\n",
      "[21]\ttrain-rmse:0.811559+0.00144161\ttest-rmse:0.825399+0.00581488\n",
      "[22]\ttrain-rmse:0.810251+0.00139159\ttest-rmse:0.824749+0.0058063\n",
      "[23]\ttrain-rmse:0.80925+0.00157431\ttest-rmse:0.82434+0.00571854\n",
      "[24]\ttrain-rmse:0.808205+0.00164857\ttest-rmse:0.824092+0.00572296\n",
      "[25]\ttrain-rmse:0.807416+0.00153973\ttest-rmse:0.82386+0.00571009\n",
      "[26]\ttrain-rmse:0.806578+0.00153374\ttest-rmse:0.823779+0.00578421\n",
      "[27]\ttrain-rmse:0.805686+0.00149918\ttest-rmse:0.823749+0.0057299\n",
      "[28]\ttrain-rmse:0.805092+0.00155066\ttest-rmse:0.823709+0.00569609\n",
      "[29]\ttrain-rmse:0.804311+0.00156563\ttest-rmse:0.823812+0.00576036\n",
      "[30]\ttrain-rmse:0.803794+0.00162291\ttest-rmse:0.823813+0.00581434\n",
      "[31]\ttrain-rmse:0.803183+0.00162875\ttest-rmse:0.823867+0.00576333\n",
      "[32]\ttrain-rmse:0.802627+0.00160897\ttest-rmse:0.823956+0.00574859\n",
      "[33]\ttrain-rmse:0.801994+0.00169942\ttest-rmse:0.823933+0.00575217\n",
      "[34]\ttrain-rmse:0.801466+0.0016519\ttest-rmse:0.823985+0.00574561\n",
      "[35]\ttrain-rmse:0.800872+0.00161262\ttest-rmse:0.824115+0.00582221\n",
      "[36]\ttrain-rmse:0.800313+0.00154208\ttest-rmse:0.82417+0.00573418\n",
      "[37]\ttrain-rmse:0.799738+0.00160132\ttest-rmse:0.824267+0.00564152\n",
      "[38]\ttrain-rmse:0.799142+0.00150995\ttest-rmse:0.824334+0.00563461\n",
      "[39]\ttrain-rmse:0.798471+0.00148018\ttest-rmse:0.824543+0.00568159\n",
      "[40]\ttrain-rmse:0.797879+0.00144345\ttest-rmse:0.824684+0.00562285\n",
      "[41]\ttrain-rmse:0.797285+0.0015082\ttest-rmse:0.824715+0.00559591\n",
      "[42]\ttrain-rmse:0.796671+0.0014529\ttest-rmse:0.824873+0.00558484\n",
      "[43]\ttrain-rmse:0.796176+0.00153688\ttest-rmse:0.824922+0.00569792\n",
      "[44]\ttrain-rmse:0.795626+0.00152949\ttest-rmse:0.825017+0.00570645\n",
      "[45]\ttrain-rmse:0.795084+0.0015928\ttest-rmse:0.825091+0.00574752\n",
      "[46]\ttrain-rmse:0.794498+0.00158684\ttest-rmse:0.825197+0.00577096\n",
      "[47]\ttrain-rmse:0.793861+0.00165386\ttest-rmse:0.825333+0.00576939\n",
      "[48]\ttrain-rmse:0.793264+0.00160505\ttest-rmse:0.825406+0.00581605\n",
      "[49]\ttrain-rmse:0.792618+0.00161954\ttest-rmse:0.825579+0.00571404\n",
      "[50]\ttrain-rmse:0.792123+0.00165671\ttest-rmse:0.825644+0.00571734\n",
      "[51]\ttrain-rmse:0.791438+0.00161541\ttest-rmse:0.825675+0.00570087\n",
      "[52]\ttrain-rmse:0.790877+0.00164572\ttest-rmse:0.825744+0.00574112\n",
      "[53]\ttrain-rmse:0.790358+0.00156286\ttest-rmse:0.825872+0.00579517\n",
      "[54]\ttrain-rmse:0.789793+0.0015843\ttest-rmse:0.825894+0.00577039\n",
      "[55]\ttrain-rmse:0.789224+0.00166944\ttest-rmse:0.825969+0.0057314\n",
      "[56]\ttrain-rmse:0.7888+0.00165974\ttest-rmse:0.825918+0.00570338\n",
      "[57]\ttrain-rmse:0.788198+0.00167104\ttest-rmse:0.825951+0.00567761\n",
      "[58]\ttrain-rmse:0.787667+0.00171939\ttest-rmse:0.826021+0.00574196\n",
      "[59]\ttrain-rmse:0.787189+0.00161553\ttest-rmse:0.826164+0.00571208\n",
      "[60]\ttrain-rmse:0.786554+0.00160438\ttest-rmse:0.826273+0.00572178\n",
      "[61]\ttrain-rmse:0.785982+0.00165973\ttest-rmse:0.826301+0.00572614\n",
      "[62]\ttrain-rmse:0.785411+0.00160538\ttest-rmse:0.826375+0.00574874\n",
      "[63]\ttrain-rmse:0.784924+0.00158223\ttest-rmse:0.826459+0.00577925\n",
      "[64]\ttrain-rmse:0.784267+0.00160097\ttest-rmse:0.826578+0.00584792\n",
      "[65]\ttrain-rmse:0.783642+0.00158109\ttest-rmse:0.826655+0.00584057\n",
      "[66]\ttrain-rmse:0.783198+0.00154037\ttest-rmse:0.826747+0.0059316\n",
      "[67]\ttrain-rmse:0.78273+0.00151783\ttest-rmse:0.826804+0.00597627\n",
      "[68]\ttrain-rmse:0.782174+0.00150134\ttest-rmse:0.82701+0.00601552\n",
      "[69]\ttrain-rmse:0.781562+0.00142021\ttest-rmse:0.827195+0.00600313\n",
      "[70]\ttrain-rmse:0.780985+0.0015004\ttest-rmse:0.827235+0.00602796\n",
      "[71]\ttrain-rmse:0.780443+0.00148186\ttest-rmse:0.827268+0.00600959\n",
      "[72]\ttrain-rmse:0.779911+0.00156984\ttest-rmse:0.827378+0.00598444\n",
      "[73]\ttrain-rmse:0.779353+0.00164955\ttest-rmse:0.827517+0.00597429\n",
      "[74]\ttrain-rmse:0.778842+0.00175115\ttest-rmse:0.827603+0.00603149\n",
      "[75]\ttrain-rmse:0.778252+0.00164307\ttest-rmse:0.827709+0.00599257\n",
      "[76]\ttrain-rmse:0.777765+0.00174268\ttest-rmse:0.827732+0.00599436\n",
      "[77]\ttrain-rmse:0.777212+0.0017013\ttest-rmse:0.827836+0.00597823\n"
     ]
    }
   ],
   "source": [
    "cvresult = xgb.cv(xgb_params, xgtrain, num_boost_round=1000, nfold=5, metrics='rmse', early_stopping_rounds=50, verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2.23724\n",
      "[20]\ttrain-rmse:0.853767\n",
      "[40]\ttrain-rmse:0.809765\n",
      "[60]\ttrain-rmse:0.802667\n",
      "[80]\ttrain-rmse:0.79628\n",
      "[100]\ttrain-rmse:0.790567\n",
      "[119]\ttrain-rmse:0.784607\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(xgb_params, xgtrain, num_boost_round=num_rounds, evals = [(xgtrain, 'train')], verbose_eval=20)\n",
    "# make prediction\n",
    "y_pred1 = model.predict(xgtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67033456426278348"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_pred1, ly_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
