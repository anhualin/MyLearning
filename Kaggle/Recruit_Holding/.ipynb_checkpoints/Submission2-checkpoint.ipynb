{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea:\n",
    "Given $k$, build a model to predict the number of visitors after k days using the following features:\n",
    "1. (holidayflag, day_of_week, is_closed, #visitors) for the past n weeks.\n",
    "2. store_id, gentre, area\n",
    "\n",
    "Only do label encoding to categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Linux':\n",
    "    data_dir = '/home/alin/Data/Recruit_Holding'\n",
    "else:\n",
    "    data_dir = 'C:/Users/alin/Documents/Data/Recruit_Holding'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load previous dump\n"
     ]
    }
   ],
   "source": [
    "DFS_dump = data_dir + '/DFS.p'\n",
    "if Path(DFS_dump).is_file():\n",
    "    print('load previous dump')\n",
    "    DFS = pickle.load(open(DFS_dump, 'rb'))\n",
    "    air_reserve = DFS['air_reserve']\n",
    "    air_reserve_day = DFS['air_reserve_day']\n",
    "    hpg_reserve = DFS['hpg_reserve']\n",
    "    hpg_reserve_day = DFS['hpg_reserve_day']\n",
    "    air_visit_hist = DFS['air_visit_hist']\n",
    "    date_info = DFS['date_info']\n",
    "    test = DFS['test']\n",
    "    air_store_info = DFS['air_store_info']\n",
    "    hpg_store_info = DFS['hpg_store_info']\n",
    "    store_id_relation = DFS['store_id_relation']\n",
    "    test = DFS['test']\n",
    "else:\n",
    "    print('run EDA1 first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the training and testing datasets before label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: add dates when a store is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(k = 3):\n",
    "    '''\n",
    "    Keep the last k weeks of air_vist_hist, then for any store missing on any day,  create the corresponding \n",
    "    row with expacted valud 0\n",
    "    '''\n",
    "    last_train_day = max(air_visit_hist.day_ind)\n",
    "    first_train_day = last_train_day - k * 7 + 1\n",
    "    \n",
    "    #filter into desire time frame\n",
    "    hist1 = air_visit_hist[(air_visit_hist.day_ind >= first_train_day) & (air_visit_hist.day_ind <= last_train_day)].copy()\n",
    "    all_stores = hist1.air_store_id.unique()\n",
    "    all_days = [i for i in range(first_train_day, last_train_day+1)]\n",
    "    \n",
    "    #create store x day grid\n",
    "    grid = np.array(list(product(*[all_stores, all_days])))\n",
    "    grid = pd.DataFrame(grid, columns=['air_store_id', 'day_ind_str' ])\n",
    "    grid['day_ind'] = grid.apply(lambda r: int(r['day_ind_str']), axis=1)\n",
    "    grid.drop('day_ind_str', axis=1, inplace=True)\n",
    "    \n",
    "    # add visit information \n",
    "    all_data = grid.merge(hist1, how='left', on=['air_store_id', 'day_ind'])\n",
    "    \n",
    "    # add date type information\n",
    "    all_data = all_data.merge(date_info, on='day_ind', suffixes=['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # add store information\n",
    "    all_data = all_data.merge(air_store_info, on = 'air_store_id', suffixes = ['_l', ''])\n",
    "    drop_columns = [col for col in all_data.columns if col[-1] == 'l'] + ['calendar_date', 'date', 'latitude', 'longitude', 'hpg_store_id']\n",
    "    all_data.drop(drop_columns, inplace=True, axis=1)\n",
    "    \n",
    "    # for those dates on which the visit informaiton of a store is missing, assume that it was closed abd with visit number 0\n",
    "    all_data['closed'] = all_data.apply(lambda r: 1 if pd.isnull(r['visitors']) else 0, axis=1)\n",
    "    all_data.fillna(0, inplace=True)\n",
    "    return all_data\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = get_grid(k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  create data frames with lag information\n",
    "\n",
    "Given gap, create training set with lag_gap, lagp_(gap+1) ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_lag(grid, lag_begin, lag_end ):\n",
    "    ''' \n",
    "    Add lag information to  grid to create training set\n",
    "    Specifically, given a row with day_ind = D, and lag_begin = 7, lag_end = 14\n",
    "    we add lag_7, lag_8, ..., lag_14 to this row   \n",
    "    \n",
    "    This is used to traing a model to forecast the visitors lag_begin days in the future\n",
    "    '''\n",
    "    index_cols = ['air_store_id', 'day_ind']\n",
    "    cols_to_rename = ['visitors', 'day_of_week', 'holiday_flg', 'holiday_eve', 'closed']\n",
    "    \n",
    "    grid_cp = grid.copy()\n",
    "    for day_shift in range(lag_begin, lag_end + 1):\n",
    "        grid_shift = grid[index_cols + cols_to_rename].copy()\n",
    "        grid_shift['day_ind'] = grid_shift['day_ind'] + day_shift   \n",
    "        foo = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=foo)\n",
    "        grid = pd.merge(grid, grid_shift, on=index_cols, how='left')\n",
    "    grid_train = grid[~pd.isnull(grid['visitors_lag_' + str(lag_end)])]\n",
    "    grid_train = grid_train[grid_train['closed'] != 1]\n",
    "    grid_train.drop(['day_ind', 'month_ind', 'closed'], axis=1, inplace=True)\n",
    "    max_day_ind = np.max(grid.day_ind)\n",
    "    grid_test = grid_cp[grid_cp.day_ind == max_day_ind]\n",
    "    \n",
    "    f = lambda x: '{}_lag_{}'.format(x, str(lag_begin)) if x in cols_to_rename else x\n",
    "    grid_test = grid_test.rename(columns=f)\n",
    "  \n",
    "    for day_shift in range(lag_begin + 1, lag_end + 1):\n",
    "        grid_shift = grid_cp[grid_cp.day_ind == (max_day_ind - day_shift + lag_begin)][['air_store_id'] + cols_to_rename].copy()\n",
    "        f = lambda x: '{}_lag_{}'.format(x, day_shift) if x in cols_to_rename else x\n",
    "        grid_shift = grid_shift.rename(columns=f)\n",
    "        grid_test = pd.merge(grid_test, grid_shift, on='air_store_id')\n",
    "        \n",
    "    grid_test.drop(['day_ind', 'month_ind'], axis=1, inplace=True)\n",
    "    return grid_train, grid_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_begin, lag_end = 3, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtrain, gtest = append_lag(grid, lag_begin, lag_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = gtrain.visitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0 = gtrain[gtest.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test0 = gtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['air_store_id', 'air_genre_name', 'air_area_name']  + ['day_of_week_lag_' +  str(lag) \n",
    "                                                                      for lag in range(lag_begin, lag_end + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatLabler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_cols):\n",
    "        self.cat_cols = cat_cols\n",
    "    def fit(self, X, y=None):\n",
    "        encoders = {}\n",
    "        for col in self.cat_cols:\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(X[col])\n",
    "            encoders[col] = encoder\n",
    "        self.encoders = encoders\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_new = X.copy()\n",
    "        for col in self.cat_cols:\n",
    "            X_new[col] = self.encoders[col].transform(X[col])\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "catLabler = CatLabler(cat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CatLabler(cat_cols=['air_store_id', 'air_genre_name', 'air_area_name', 'day_of_week_lag_3', 'day_of_week_lag_4', 'day_of_week_lag_5'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catLabler.fit(X_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = catLabler.transform(X_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = catLabler.transform(X_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(k=10, lag_begin0=1, lag_begin1=39):\n",
    "    '''\n",
    "    input:\n",
    "    k -- create k weeks grid starting from the last date in air_visit_hist\n",
    "    lag: for each LAG between lag_begin0 and lag_begin1, create train set\n",
    "    X_train_lag and y_train_lag where X_train has lag from LAG to 7 * k - 1,\n",
    "    also create a test set X_test_lag \n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
